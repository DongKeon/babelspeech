{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 바벨스피치 Part2 스터디\n",
    "## 파트 2 - 3회차\n",
    "\n",
    "### (Deep NLP) Lecture 5 - Text Classification 이상열\n",
    "\n",
    "-----------------\n",
    "커리큘럼\n",
    "* (딥NLP 기초) Deep Learning for Natural Language Processing -https://github.com/oxford-cs-deepnlp-2017/lectures\n",
    "* (음성인식 기초) CS224S / LINGUIST285 - Spoken Language Processing -https://web.stanford.edu/class/cs224s/\n",
    "* (텐서플로우 기초) Machine Learning with TensorFlow http://www.tensorflowbook.com/\n",
    "* (참고 자료) 텐서플로우 실습 코드 https://github.com/BinRoot/TensorFlow-Book\n",
    "\n",
    "\n",
    "### github 레퍼지토리 : https://github.com/KaggleBreak/babelspeech\n",
    "\n",
    "### 스터디 그룹 바벨피쉬 https://www.facebook.com/groups/babelPish/\n",
    "### 스터디 그룹 캐글뽀개기 https://www.facebook.com/groups/kagglebreak/\n",
    "\n",
    "-----------------\n",
    "\n",
    "\n",
    "## Deep Learning for Natural Language Processing Text Classification\n",
    "- DeepMind, Karl Moritz Hermann\n",
    "\n",
    "-----------------\n",
    "\n",
    "\n",
    "### This lecture discusses text classification. As part of that we’ll discuss the following\n",
    "- Generative and discriminative models\n",
    "- Naive Bayes\n",
    "- Logistic regression\n",
    "- Text representation (BOW, Features, RNNs, Convolutions) \n",
    "- Softmax Classifiers\n",
    "- Practical Aspects of Classifier Training\n",
    "\n",
    "-----------------\n",
    "\n",
    "### Classification Tasks\n",
    "- Is this e-mail spam?\n",
    "- Positive or negative review?\n",
    "- What is the topic of this article? • Predict hashtags for a tweet\n",
    "- Age/gender identification\n",
    "- Language identification\n",
    "- Sentiment analysis\n",
    "- ...\n",
    "\n",
    "-----------------\n",
    "\n",
    "### Types of Classification Tasks\n",
    "- Binary classification (true, false)\n",
    "- Multi-class classification (politics, sports, gossip)\n",
    "- Multi-label classification (#party #FRIDAY #fail) \n",
    "- Clustering (labels unknown)\n",
    "\n",
    "-----------------\n",
    "\n",
    "### Classification Methods\n",
    "\n",
    "- By hand\n",
    "    - E.g. Yahoo in the old days\n",
    "    - Very accurate and consistent assuming experts 8 Super slow, expensive, does not scale\n",
    "\n",
    "- Rule-based\n",
    "    - E.g. Advanced search criteria (”site:ox.ac.uk”)\n",
    "    - Accuracy high if rule is suitable\n",
    "    - Need to manually build and maintain rule-based system.\n",
    "\n",
    "- Statistical\n",
    "    - Scales well, can be very accurate, automatic\n",
    "    - Requires classified training data. Sometimes a lot!\n",
    "\n",
    "-----------------\n",
    "\n",
    "### Statistical Text Classification\n",
    "- Assume some text represented by d and some class c. We want to learn the probability of d being of class c:\n",
    "\n",
    "- Key questions:\n",
    "    - How to represent d.\n",
    "    - How to calculate P(c|d)\n",
    "    \n",
    "    \n",
    "-----------------\n",
    "\n",
    "### Text Classification in Two Parts\n",
    "- Think of text classification as a two stage process:\n",
    "\n",
    "- Representation\n",
    "    - Process text into some (fixed) representation.\n",
    "    - How to learn d\n",
    "\n",
    "- Classification\n",
    "    - Classify document given that representation.\n",
    "    - How to learn P(c|d)\n",
    "    \n",
    "    \n",
    "-----------------\n",
    "\n",
    "### Possible Representations for Text\n",
    "\n",
    "- Bag of Words (BOW)\n",
    "    - Easy, no e↵ort required.\n",
    "    - Variable size, ignores sentential structure.\n",
    "\n",
    "- Hand-crafted features\n",
    "    - Full control, can use of NLP pipeline, class-specific features \n",
    "    - Over-specific, incomplete, makes use of NLP pipeline.\n",
    "\n",
    "- Learned feature representation\n",
    "    - Can learn to contain all relevant information. \n",
    "    - Needs to be learned.\n",
    "    \n",
    "\n",
    "-----------------\n",
    "\n",
    "### Generative vs. Discriminative Models\n",
    "\n",
    "- Generative (joint) models\n",
    "    - P(c,d)\n",
    "    -  Model the distribution of individual classes and place probabilities over both observed data and hidden variables (such as labels)\n",
    "    - E.g. n-gram models, hidden Markov models, probabilistic context-free grammars, IBM machine translation models, Na ̈ıve Bayes, ...\n",
    "\n",
    "- Discriminative (conditional) models \n",
    "    - P(c|d) \n",
    "    - Learn boundaries between classes. Take data as given and put probability over the hidden structure given the data.\n",
    "    - E.g. logistic regression, maximum entropy models, conditional random fields, support-vector machines, ...\n",
    "    \n",
    "\n",
    "-----------------\n",
    "\n",
    "### Naive Bayes classifier (1/4)\n",
    "\n",
    "![img1](./img/img1.png)\n",
    "![img2](./img/img2.png)\n",
    "![img3](./img/img3.png)\n",
    "\n",
    "-----------------\n",
    "\n",
    "### Naive Bayes classifier (4/4)\n",
    "\n",
    "- Advantages\n",
    "    - Simple\n",
    "    - Interpretable\n",
    "    - Fast (linear in size of training set and test document) • Text representation trivial (bag of words)\n",
    "\n",
    "- Drawbacks\n",
    "    - Independence assumptions often too strong\n",
    "    - Sentence/document structure not taken into account\n",
    "    - Naive classifier has zero probabilities; smoothing is awkward\n",
    "\n",
    "-----------------\n",
    "\n",
    "### Quiz\n",
    "- Is Naive Bayes a generative or a discriminative model?\n",
    "\n",
    "\n",
    "-----------------\n",
    "\n",
    "### Quiz\n",
    "- Naive Bayes is a generative model!\n",
    "- P(c|d) = P(d|c)P(c) P(d)\n",
    "- P(c|d)P(d) = P(d|c)P(c) = P(d, c)\n",
    "    - While we use a conditional probability P(c|d) for classification, we model the joint probability of c and d.\n",
    "    - This means it is trivial to invert the process and generate new text given a class label.\n",
    "\n",
    "-----------------\n",
    "\n",
    "### Feature Representations\n",
    "\n",
    "- A feature representation (of text) can be viewed as a vector where each element indicates the presence or absence of a given feature in a document.\n",
    "- Note: features can be binary (presence/absence), multinomial (count) or continuous (eg. TF-IDF weighted).\n",
    "\n",
    "![img4](./img/img4.png)\n",
    "\n",
    "- What does this feature representation classify?\n",
    "\n",
    "\n",
    "-----------------\n",
    "\n",
    "### Logistic Regression (1/7)\n",
    "\n",
    "- If we only want to classify text, we do not need the full power of a generative model, but a discriminative model is suffcient.\n",
    "\n",
    "- We only want to learn P(c|d).\n",
    "- A general framework for this is logistic regression. logistic because is uses a logistic function\n",
    "- regression combines a feature vector (d) with weights (w) to compute an answer\n",
    "\n",
    "\n",
    "![img5](./img/img5.png)\n",
    "\n",
    "![img6](./img/img6.png)\n",
    "\n",
    "![img7](./img/img7.png)\n",
    "\n",
    "\n",
    "-----------------\n",
    "\n",
    "### Logistic Regression (5/7)\n",
    "\n",
    "-  Given this model formulation, we want to learn parameters   that maximise the conditional likelihood of the data according to the model.\n",
    "- Due to the softmax function we not only construct a classifier, but learn probability distributions over classifications.\n",
    "- There are many ways to chose weights  :\n",
    "    - Perceptron : Find misclassified examples and move weights in the direction of their correct class \n",
    "    - Margin-Based Methods such as Support Vector Machines can be used for learning weights\n",
    "    - Logistic Regression Directly maximise the conditional log-likelihood via gradient descent (next slide).\n",
    "\n",
    "\n",
    "![img8](./img/img8.png)\n",
    "\n",
    "\n",
    "-----------------\n",
    "\n",
    "### Logistic Regression (7/7)\n",
    "\n",
    "- Advantages\n",
    "    - Still reasonably simple\n",
    "    - Results are very interpretable\n",
    "    - Do not assume statistical independence between features!\n",
    "\n",
    "- Drawbacks\n",
    "    - Harder to learn than Na ̈ıve Bayes\n",
    "    - Manually designing features can be expensive\n",
    "    - Will not necessarily generalise well due to hand-crafted features\n",
    "    \n",
    "\n",
    "-----------------\n",
    "\n",
    "### Recap: Recurrent Neural Networks\n",
    "\n",
    "- Recurrent Neural Network Language Model\n",
    "\n",
    "![img9](./img/img9.png)\n",
    "\n",
    "- Agnostic to actual recurrent function (LSTM, GRU, ..)\n",
    "- Reads inputs xi to accumulate state hi and predict outputs yi\n",
    "\n",
    "\n",
    "-----------------\n",
    "\n",
    "### Information contained in an RNN\n",
    "\n",
    "![img9](./img/img9.png)\n",
    "\n",
    "- Distribution over output (next word)\n",
    "- Accumulated state captures {x_0, ..., x_i}\n",
    "\n",
    "\n",
    "-----------------\n",
    "\n",
    "### Representing Text with an RNN\n",
    "\n",
    "- hi is a function of x{0:i} and h{0:i 1}\n",
    "-  It contains information about all text read up to point i. \n",
    "- The first half of this lecture was focused on learning a representation X for a given text \n",
    "- h is precisely that\n",
    "\n",
    "\n",
    "-----------------\n",
    "\n",
    "### Logistic Regression Revisited\n",
    "\n",
    "- Effectively, X = hn where n is the length of a given input document.\n",
    "- So in order to classify text we can simply take a trained language model (last week) and extract text representations from the final hidden state cn.\n",
    "- Classification as before using a logistic regression:\n",
    "\n",
    "![img10](./img/img10.png)\n",
    "\n",
    "- Can use RNN + Logistic Regression out of the box\n",
    "- Can in fact use any other classifier on top of h!\n",
    "- (X) How to ensure that h pays attention to relevant aspects of data?\n",
    "\n",
    "\n",
    "-----------------\n",
    "\n",
    "### Text Classification with an RNN\n",
    "\n",
    "- Move the classification function inside the network\n",
    "\n",
    "![img11](./img/img11.png)\n",
    "\n",
    "\n",
    "-----------------\n",
    "\n",
    "### Text Classification with an RNN\n",
    "\n",
    "![img12](./img/img12.png)\n",
    "\n",
    "\n",
    "-----------------\n",
    "\n",
    "### Loss function for an RNN Classifier\n",
    "\n",
    "- You will have encountered this classifier already in the first lecture. This is a simple Multilayer Perceptron (MLP).\n",
    "- We can train the model using the cross-entropy loss:\n",
    "\n",
    "- Cross-entropy is designed to deal with errors on probabilities.\n",
    "- Optimizing means minimizing the cross-entropy between the estiated class probabilities (p(c|d) and the true distribution.\n",
    "- There are many alternative losses (hinge-loss, square error, L1 loss) not discussed today.\n",
    "\n",
    "-----------------\n",
    "\n",
    "### Extension: Multi-Label Classification\n",
    "\n",
    "- What if a single data point may have multiple correct labels?\n",
    "- (X) The cross-entropy loss assumes there is only one correct label. Option 1: Binary classifiers\n",
    "\n",
    "#### Option 1: Binary classifiers\n",
    "- For each possible label class (c) define a binary classifier (true/false) on class feature weights mc . Assume yic = 1 if example i is in class c and 0 otherwise. The loss per example i is:\n",
    "\n",
    "![img13](./img/img13.png)\n",
    "\n",
    "- How else could you formulate a multi-label classifier?\n",
    "\n",
    "\n",
    "-----------------\n",
    "\n",
    "### Dual Objective RNN\n",
    "\n",
    "- In practice it may make sense to combine an LM objective with classifier training and to optimise the two losses jointly.\n",
    "\n",
    "![img9](./img/img11.png)\n",
    "\n",
    "![img14](./img/img14.png)\n",
    "\n",
    "- Such a joint loss enables making use of text beyond labelled data.\n",
    "\n",
    "\n",
    "-----------------\n",
    "\n",
    "### Bi-Directional RNNs\n",
    "\n",
    "- Another way to add signal is to process the input text both in a forward and in a backward sequence.\n",
    "\n",
    "![img15](./img/img15.png)\n",
    "\n",
    "- The update rules for this directly follow the regular forward-facing RNN architecture. In practice, bidirectional networks have shown to be more robust than unidirectional networks.\n",
    "\n",
    "\n",
    "-----------------\n",
    "\n",
    "### Bi-Directional RNNs\n",
    "\n",
    "![img16](./img/img16.png)\n",
    "\n",
    "- A bidirectional network can be used as a classifier simply by redefining d to be the concatenation of both final hidden states:\n",
    "\n",
    "- d = ( hn->|| k || h0<-)\n",
    "\n",
    "\n",
    "-----------------\n",
    "\n",
    "### Quiz\n",
    "\n",
    "- Question : Is a simple RNN classifier a generative or discriminative model?\n",
    "\n",
    "\n",
    "-----------------\n",
    "\n",
    "### Quiz\n",
    "\n",
    "- Question : Is a simple RNN classifier a generative or discriminative model?\n",
    "    - RNN Classifiers can be either!\n",
    "    \n",
    "![img17](./img/img17.png)\n",
    "\n",
    "- Encoder: discriminative (it does not model the probability of the text)\n",
    "- Joint-model: generative (learns both P(c) and P(d))\n",
    "\n",
    "\n",
    "-----------------\n",
    "\n",
    "### Non-Sequential Neural Networks\n",
    "\n",
    "- Outside of natural language a lot of data is not sequential. There are different architectures designed to deal with such data.\n",
    "- Convolutional Neural Networks were designed for image classification but can be adapted to work for language, too.\n",
    "- Recursive Neural Networks are a language-centric variant that have found success particularly for classification tasks.\n",
    "\n",
    "-----------------\n",
    "\n",
    "### Recursive Neural Networks\n",
    "\n",
    "- Composition follows syntactic structure\n",
    "    - Accumulation of state follows syntax \n",
    "    - Can chose any form of tree structure\n",
    "\n",
    "![img18](./img/img18.png)\n",
    "\n",
    "\n",
    "-----------------\n",
    "\n",
    "### Recursive Neural Networks\n",
    "\n",
    "- While recursive networks have no simple generative counterpart, it is possible to improve classifier training by adding an additional autoencoder loss.\n",
    "\n",
    "- Autoencoder Signals\n",
    "\n",
    "![img19](./img/img19.png)\n",
    "\n",
    "\n",
    "-----------------\n",
    "\n",
    "### Convolutional Network Networks\n",
    "\n",
    "- For images Convolutional Neural Networks are very useful:\n",
    "\n",
    "![img20](./img/img20.png)\n",
    "\n",
    "- Convolution : iteratives takes a matrix of inputs and computes a (smaller) matrix of outputs\n",
    "- Subsampling : takes a matrix of inputs and pools those into a single element , e.g. by taking the maximum\n",
    "\n",
    "-----------------\n",
    "\n",
    "### Convolutional Network Networks\n",
    "\n",
    "- Convolutional window acts as a classifer for local features\n",
    "\n",
    "![img21](./img/img21.png)\n",
    "\n",
    "-----------------\n",
    "\n",
    "### Convolutional Network Networks\n",
    "\n",
    "- Reasons to consider CNNs for Text\n",
    "    - Really fast (GPUs! See Thursday)\n",
    "    - BOW is often su cient (see Na ̈ıve Bayes)\n",
    "    - Actually can take some structure into account\n",
    "    - (X) Not sequential in its processing of input data\n",
    "    - (X) Easier to discriminate than to generate variably sized data\n",
    "    \n",
    "![img22](./img/img22.png)\n",
    "\n",
    "\n",
    "-----------------\n",
    "\n",
    "### Convolutional Network Networks\n",
    "\n",
    "![img23](./img/img23.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
