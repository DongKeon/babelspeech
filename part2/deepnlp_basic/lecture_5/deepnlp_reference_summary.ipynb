{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 바벨스피치 Part2 스터디\n",
    "## 파트 2 - 3회차\n",
    "\n",
    "### (Deep NLP) Lecture 5 - Text Classification 이상열\n",
    "\n",
    "-----------------\n",
    "커리큘럼\n",
    "* (딥NLP 기초) Deep Learning for Natural Language Processing -https://github.com/oxford-cs-deepnlp-2017/lectures\n",
    "* (음성인식 기초) CS224S / LINGUIST285 - Spoken Language Processing -https://web.stanford.edu/class/cs224s/\n",
    "* (텐서플로우 기초) Machine Learning with TensorFlow http://www.tensorflowbook.com/\n",
    "* (참고 자료) 텐서플로우 실습 코드 https://github.com/BinRoot/TensorFlow-Book\n",
    "\n",
    "\n",
    "### github 레퍼지토리 : https://github.com/KaggleBreak/babelspeech\n",
    "\n",
    "### 스터디 그룹 바벨피쉬 https://www.facebook.com/groups/babelPish/\n",
    "### 스터디 그룹 캐글뽀개기 https://www.facebook.com/groups/kagglebreak/\n",
    "\n",
    "-----------------\n",
    "\n",
    "### 블로그1 : Understanding Convolutional Neural Networks for NLP (http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/)\n",
    "\n",
    "- 이미지 픽셀 대신 대부분의 NLP 작업에 대한 입력은 행렬로 표시된 문장 또는 문서입니다\n",
    "- 행은 단어를 나타내는 벡터입니다. 일반적으로 이러한 벡터는 word2vec 또는 GloVe와 같은 단어 삽입 (저 차원 표현)이지만 단어를 어휘로 색인화하는 단일 핫 벡터일 수도 있습니다.\n",
    "- 100 차원 임베딩을 사용하는 10 개의 단어 문장에 대해 우리는 입력으로 10 × 100 행렬을 가질 것입니다. 그것은 우리의 \"이미지\"입니다.\n",
    "\n",
    "\n",
    "- 비전에서 필터는 이미지의 로컬 패치를 따라 움직이지만 NLP에서는 일반적으로 행렬의 전체 행 (단어) 위로 밀려 오는 필터를 사용합니다. 따라서 필터의 \"너비\"는 일반적으로 입력 행렬의 너비와 같습니다. 높이 또는 지역 크기는 다를 수 있지만 한 번에 2-5 단어 이상의 슬라이딩 창이 일반적입니다. \n",
    "- 위치 불변성과 지역 구성은 이미지에 대해 직관적이었지만 NLP에는 그다지 중요하지 않았습니다. 문장에서 단어가 나오는 부분을 많이 처리해야 할 것입니다. (가깝에 있는 픽셀은 의미론적으로 관련이 있지만 단어는 그렇지 않음)\n",
    "- 간단한 Bag of Words 모델은 틀린 가정을 가진 명백한 단순화이지만, 그럼에도 불구하고 수년간 표준 접근법이었으며 꽤 좋은 결과를 이끌어 낸거처럼 CNN도 그러합니다.\n",
    "\n",
    "![d1_1](./img/b1_1.png)\n",
    "\n",
    "- CNN에 대한 큰 논점은 그들이 빠르다는 것입니다. 매우 빠릅니다. 컨볼 루션은 컴퓨터 그래픽의 핵심 부분이며 GPU의 하드웨어 수준에서 구현됩니다. n-grams와 비교할 때, CNN은 표현 측면에서도 효율적입니다.\n",
    "- 컨볼루션 필터는 전체 어휘를 나타낼 필요없이 자동으로 훌륭한 표현을 학습합니다\n",
    "\n",
    "- zero-padding를 추가하는 것이 wide convoluiton, 추가하지 않는 것이 narrow convoluiton이라고 부름, 아래는 1차원에서 예제입니다.\n",
    "- 입력 크기와 관련하여 큰 필터가 있을 때 Wide 컨볼루션이 유용합니다. 아래의 경우,  narrow 컨볼루션은 크기 (7-5) + 1 = 3의 출력을 가져오며, wide 컨볼루션은 크기 (7 + 2 * 4-5) + 1 = 11의 출력을 산출합니다. \n",
    "\n",
    "![d1_1](./img/b1_2.png)\n",
    "\n",
    "- NLP에서 풀링은 전체 출력에 대해 적용하고 각 필터별로 하나의 숫자만 나온다\n",
    "- 예를 들어 1,000개의 필터를 가지고 있고 각각 맥스 풀링하면 필터의 크기와 입력의 크기와 상관없이 1,000차원의 출력을 얻게 된다.\n",
    "- 가변크기의 문장,필터를 사용하지만 분류기에 넣을 떄는 같은 크기의 출력 차원을 얻게 해준다.\n",
    "- 풀링은 출력 차원을 감소하지만 알짜 정보는 유지하는데 'not amazing' 이라는 부정문을 문장이 포함하느지 찾는 것 같은 특정 피쳐를 추출해주는 것으로 생각할 수 있음\n",
    "- 다음 구절이 문장의 어느곳에 등장했다면 필터 적용 결과 큰 값을 줄 것이고 나머지 지점에서는 작은 값을 줄 것이다. max 연산을 수행하면서 특성이 문장에서 나오는지, 안나오는지 정보를 유지할 수 있음\n",
    "- 어디서 그것이 나타나는지에 대한 정보를 잃음, bag-of n-grams model과 비슷함, locality에 대한 글로벌 정보는 잃지만 필터가 잡은 amazing not과 not amazing가 다르다는 local 정보는 유지할 수 있음\n",
    "\n",
    "![d1_1](./img/b1_3.png)\n",
    "\n",
    "- Character-level CNNs의 경우 문자레벨 임베딩을 학습하고 pre-trained word 임베디등과 조인해서 음성인식 태깅을 위한 CNN에도 사용한다. \n",
    "- 사전학습된 임베딩 없이 문자로부터 직접 학습하는 CNN의 사용을 연구되기도 함 \n",
    "- 문자-레벨 입력으로부터 직접적으로 학습하는 것은 큰 데이터셋(100만)에서만 잘 동작함.\n",
    "\n",
    "-----------------\n",
    "\n",
    "### 논문명 1 : Recurrent Convolutional Neural Networks for Text Classification\n",
    "\n",
    "- 단어 표현을 학습할 때 문맥 정보를 가능한 한 많이 포착하기 위해 전통적인 윈도우 기반 신경망 네트워크에 비해 잡음을 상당히 줄이는 양방향 반복 구조 적용\n",
    "- 텍스트 분류에서 주요 수행하는 기능으로 자동으로 판단하여 텍스트의 주요 구성 요소를 캡처하는 최대 풀링 계층을 사용합니다. \n",
    "    - 재귀적 신경망은 트리 구조의 성능으로 O(n2)의 시간 복잡도가 있어서 긴 문장이나 문서를 모델링하는데 시간 복잡도가 있어서 적합하지 않음\n",
    "    - 다만 RNN은 편항된 모델, 주요 구성 요소가 문서의 끝 부분이 아닌 문서의 어느 위치에서나 나타낼 수 있으므로 나중에 나오는 단어가 이전 단어보다 우세하게 나타냄\n",
    "    - CNN은 재귀적 신경망 또는 반복적 시경망에 비하여 텍스트의 구문 의미를 더 잘 포착할 수 있음, 다만 CNN에 대한 이전 연구는 고정 윈도우와 같은 간단한 컨볼루션 커널을 사용했고, 창 크기를 결정하기가 어려웠음 (창 크기가 작으면 일부 중요한 정보가 손실, 큰 창은 막대한 매개변수 공간을 초래함)\n",
    "    - 위 모델의 한계를 극복하기 위해 RCNN을 제안함 (반복적 구조와 최대 - 풀링 계층을 결합함으로써, 모델은 RNN 신경망 모델과 CNN 신경망 모델의 장점을 이용한다.)\n",
    "    -  RCNN 모델은 O(n)의 시간 복잡도를 보여주며 텍스트 길이와 선형적인 상관 관계가 있음\n",
    "    \n",
    "![d1_1](./img/d1_1.png)\n",
    "\n",
    "- cl(wi)를 단어 wi의 왼쪽 문맥으로, cr(wi)를 단어 wi의 오른쪽 문맥으로 정의한다. \n",
    "- cl (wi)와 cr (wi)는 |c|가 있는 조밀한 벡터이다. \n",
    "- 단어 wi의 왼쪽 문맥 cl(wi)는 등식 (1)을 사용하여 계산된다. 여기서 e(wi-1)는 단어 wi-1의 단어 임베딩이고, \n",
    "- cl (wi-1)은 이전 단어 wi-1의 왼쪽 컨텍스트이다. \n",
    "- 모든 문서에서 첫 번째 단어의 왼쪽 컨텍스트는 동일한 공유 매개 변수 cl(w1)을 사용합니다. W(l)은 숨겨진 레이어 (컨텍스트)를 다음 숨겨진 레이어로 변환하는 매트릭스입니다. \n",
    "- W(sl)는 현재 단어의 의미와 다음 단어의 왼쪽 컨텍스트를 결합하는 데 사용되는 행렬입니다. \n",
    "- f는 비선형 활성화 함수입니다. 우측 콘텍스트 cr (wi)은 식 (2)와 같이 유사한 방식으로 계산된다. 문서에서 마지막 단어의 오른쪽 컨텍스트는 매개 변수 cr (wn)을 공유합니다.\n",
    "\n",
    "![d1_3](./img/d1_3.png)\n",
    "\n",
    "- 컨텍스트 벡터는 모든 좌측 및 우측 컨텍스트의 의미를 포착한다. 단어의 의미를 명확하게 해독할 수 있음\n",
    "- 반복 구조는 텍스트의 정방향 스캔에서 모든 c1를 획득하고 텍스트의 역방향 스캔에서 c1를 다시 얻을 수 있다. 단어 wi의 표현 xi를 얻은 후에 xi의 tanh 활성화 함수와 함께 선형 변환을 적용하여 그 결과를 다음 레이어로 보낸다.\n",
    "- y는 잠재적인 의미론적 벡터이며 각각의 의미론적 요인을 분석하여 텍스트를 표현하는데 가장 유용한 요소를 결정함\n",
    "- 풀링 계층은 다양한 길이의 텍스트를 고정 길이 벡터로 변환합니다. 풀링 레이어를 사용하면 전체 텍스트에서 정보를 캡처 할 수 있습니다. \n",
    "\n",
    "![d1_4](./img/d1_4.png)\n",
    "\n",
    "- 신경 네트워크 접근법 (Recursive NN, CNN 및 RCNN)을 널리 사용되는 기존 방법 (예 : BoW + LR)과 비교할 때 실험 결과는 네 가지 데이터 세트 모두에 대해 신경망이 기존 방식보다 우위에 있음을 보여줍니다.\n",
    "- 신경망 기반 접근법이 텍스트의 일원성 표현을 효과적으로 구성 할 수 있음을 증명합니다. \n",
    "- 신경망은 BoW 모델을 기반으로하는 전통적인 방법과 비교하여 기능의 문맥 정보를 더 잘 캡슐화 할 수 있으며 데이터 희소성 문제가 덜 발생할 수 있습니다.\n",
    "\n",
    "\n",
    "![d1_5](./img/d1_5.png)\n",
    "\n",
    "- 컨볼루션 기반 프레임 워크가 이전 신경 네트워크와 비교하여 텍스트의 의미론적 표현을 구성하는 데 더 적합하다는 것을 보여줍니다.\n",
    "- 주된 이유는 CNN이 최대 풀링 레이어를 통해 차별화 된 기능을 선택하고 컨볼루션 레이어를 통해 컨텍스트 정보를 캡처 할 수 있기 때문입니다. 대조적으로, RecursiveNN은 트리 구조의 성능에 크게 의존하는 구성된 텍스트 트리 아래의 의미 구성을 사용하여 컨텍스트 정보 만 캡처 할 수 있습니다. \n",
    "- 또한, 문장의 표현을 구성하기 위해 O (n2) 시간을 필요로하는 재귀적 접근법과 비교할 때, 우리 모델은 O (n)의 시간 복잡성이 낮다. \n",
    "- 실제로, Socher et al.에서보고 된 RNTN의 훈련 시간은 다음과 같다.(2013) 소요 시간은 약 3-5 시간입니다. SST 데이터 세트에서 RCNN을 교육하는 것은 단일 스레드 시스템을 사용하는 데 수 분 밖에 걸리지 않습니다.\n",
    "\n",
    "- 문맥 정보이 서브 섹션에서는 컨텍스트 정보를 보다 자세하게 포착하기 위해 우리의 모델에 반복 구조의 능력을 조사한다. \n",
    "- CNN과 RCNN 사이의 차이점은 컨텍스트 정보를 캡처하는 데 다른 구조를 사용한다는 것입니다. \n",
    "- CNN은 고정 된 단어 창을 문맥 정보로 사용하는 반면, RCNN은 광범위한 문맥 정보를 캡쳐하기 위해 반복 구조를 사용합니다. CNN의 성능은 창 크기에 영향을받습니다. 작은 창은 일부 장거리 패턴의 손실을 초래할 수 있지만 큰 창은 데이터 희소성을 유발할 수 있습니다. 또한 많은 수의 매개 변수가 훈련하기가 더 어렵습니다.\n",
    "\n",
    "\n",
    "-----------------\n",
    "\n",
    "### 논문명 2 : A Convolutional Neural Network for Modelling Sentences \n",
    "\n",
    "- 동적 k-max 풀링 \n",
    "    - 풀링시 1 단어만 추출하면 정보 손실이 있을 수 있으므로 2단어 이상을 다음 cnn layer에 보내는 것\n",
    "    - k-Max Pooling은 Filter에서 k개의 개수만큼 Max 값을 Pooling하는 기법을 말한다. 예를 들어, k=2일 경우 Filter에 속한 값 중 가장 높은 값 2개를 Pooling한다.\n",
    "    - 최대 풀링 연산자의 일반화입니다. 최대 풀링 연산자는 값 집합의 최대 값을 반환하는 비선형 서브 샘플링 함수입니다\n",
    "\n",
    "- 동적 k-max 풀링에 의해 주어진 동적 풀링 계층과 넓은 컨볼루션 계층을 교체하는 컨볼루션 (convolutional) 아키텍처를 사용하여 문장을 모델링합니다. \n",
    "- 네트워크에서, 중간 계층에서의 피쳐 맵의 폭은 입력 문장의 길이에 따라 변한다. \n",
    "- 결과적인 아키텍처는 동적 컨볼루션 뉴럴 네트워크 (Dynamic Convolutional Neural Network)이다. 다음 그림은 DCNN을 나타냅니다. 우리는 네트워크를 자세히 설명합니다.\n",
    "\n",
    "![d2_2](./img/d2_2.png)\n",
    "\n",
    "- 와이드 컨볼루션\n",
    "    - 입력 문장이 주어지면, DCNN의 첫 번째 계층을 얻기 위해 문장의 각 단어에 대해 다음과 같은 임베디드 wi ∈ Rd를 취하여 식과 같이 문장 행렬 s ∈ Rd × s를 구성한다. \n",
    "    - 임베딩 wi의 값은 학습 중에 최적화 된 매개 변수입니다. 네트워크의 컨볼루션 계층은 가중치 행렬 m ∈ Rd × m을 아래의 계층에서 활성화 행렬과 컨벌루션함으로써 얻어진다. \n",
    "    - 예를 들어, 두 번째 계층은 문장 행렬 자체에 컨볼루션을 적용하여 구합니다. 차원 d와 필터 너비 m은 네트워크의 하이퍼 매개 변수입니다. \n",
    "    - 우리는 S작업을 1 차원 컨벌루션으로 넓게 보냅니다. 2.2. 결과 행렬 c는 d × (s + m-1) 차원을 갖는다.\n",
    "\n",
    "\n",
    "![d2_3](./img/d2_3.png)\n",
    "\n",
    "\n",
    "- k-Max 풀링\n",
    "    - Max-TDNN 문장 모델에서 사용 된 시간 차원에 대한 최대 풀링과 객체 인식을 위한 컨벌루션 네트워크에서 적용된 로컬 최대 풀링 연산과는 다른 풀링 연산을 설명한다 (LeCun et al., 1998) . pk max의 값 순서는 p의 원래 순서와 일치합니다.\n",
    "    - k-max 풀링 연산은 다수의 위치가 떨어져있을 수있는 p에서 k 개의 가장 활동적인 피쳐들을 풀링하는 것을 가능하게한다; \n",
    "    - 기능의 순서는 유지되지만 특정 위치에 영향을 받지 않습니다. \n",
    "    - 또한 피쳐가 p에서 많이 활성화 된 횟수와 피쳐의 높은 활성화가 p에서 변경되는 진행을보다 세밀하게 식별 할 수 있습니다. \n",
    "    - k-max 풀링 연산자는 최상위 콘볼 루션 계층 다음에 네트워크에 적용됩니다. \n",
    "    - 완전히 연결된 레이어에 대한 입력이 입력 문장의 길이와 무관하다는 것을 보장합니다. 그러나 다음에 볼 수 있듯이, 중간 컨볼 루션 계층에서 풀링 매개 변수 k는 고정되어 있지 않고 고차원 및 장거리 피쳐의 원활한 추출을 위해 동적으로 선택됩니다.\n",
    "    \n",
    "\n",
    "- 동적 k-Max 풀링\n",
    "    - 동적 k-max 풀링 연산은 k-max 풀링 연산이며, 여기서 k는 문장의 길이와 네트워크 깊이의 함수가 되도록 합니다.     \n",
    "\n",
    "- 폴딩\n",
    "    - 문장 행렬의 개개의 행에 적용된 특징 검출기는 다수의 지형지도에서 동일한 행에 걸쳐 많은 차수를 가지며 복잡한 의존성을 만들 수 있습니다. \n",
    "    - 그러나 서로 다른 행에 있는 피쳐 감지기는 맨 위의 완전히 연결된 레이어가 될 때까지 서로 독립적입니다. \n",
    "    - 대각선의 드문드문 한 행렬 대신에 전체 행렬. 여기에서는 추가 매개 변수를 사용하지 않는 간단한 접는 방법을 살펴 봅니다. \n",
    "    - 컨볼루션 계층 이후와 (동적) k-max 풀링 후에는 기능 맵에서 두 행을 합계로 계산합니다. d 행의 지도의 경우 접기는 d / 2 행의지도를 반환하므로 표현의 크기가 절반으로 줄어 듭니다. 폴딩 레이어를 사용하면 i 차수의 특징 검출기가 i-1 차의 하위지도에 있는 특성 값의 두 행에 종속됩니다.\n",
    "\n",
    "\n",
    "![d2_4](./img/d2_4.png)\n",
    "\n",
    "\n",
    "![d2_5](./img/d2_5.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
