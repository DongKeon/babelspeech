{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 Hidden Markov models\n",
    "\n",
    "![6_1](./img/hmk.png)\n",
    "\n",
    "- Defining interpretive models\n",
    "- Using Markov chains to model data\n",
    "- Inferring hidden state using a Hidden Markov Model\n",
    "\n",
    "- 참고 자료\n",
    "[1] - [machine learning with tensorflow](https://www.manning.com/books/machine-learning-with-tensorflow?a_aid=TensorFlow&a_bid=042443a4)\n",
    "[2] - [aailab kaist youtube](https://www.youtube.com/channel/UC9caTTXVw19PtY07es58NDg)\n",
    "[3] - [ratsgo's blog Reference](https://ratsgo.github.io/machine%20learning/2017/03/18/HMMs/)\n",
    "\n",
    "\n",
    "### HMM 소개\n",
    "\n",
    "![img 1](./img/hmm_1.png)\n",
    "\n",
    "![img 2](./img/hmm_2.png)\n",
    "\n",
    "![img 3](./img/dyna_1.png)\n",
    "\n",
    "\n",
    "### 설명\n",
    "\n",
    "- 보일의 법칙 (가스의 압력과 부피는 고정된 온도에서 반비례 관계에 있음)\n",
    "- 세상에 대해 발견된 이 간단한 법칙들로부터 통찰력 있는 추론을 할 수 있음. 최근에는 기계 학습이 연역적 추론에 중요한 측면의 역할\n",
    "- \"로켓과학\"과 \"기계학습\"은 일반적으로 함께 나타나는 문구가 아닙니다. 그러나 요즘에는 지능형 데이터 기반 알고리즘을 사용하여 실제 센서 판독 값을 모델링하는 것이 항공 우주 산업에서 쉽게 접근 할 수 있습니다. 또한, 기계학습 기술의 사용은 의료 및 자동차 산업에서 번성하고 있습니다.\n",
    "\n",
    "[한국형 시험발사체](http://m.news.naver.com/read.nhn?mode=LSD&sid1=105&sid2=230&oid=016&aid=0001336448)\n",
    "\n",
    "- 유입 원인 중 일부는 학습 매개 변수가 명확한 해석을 하는 기계학습 모델인 해석가능 모델을 더 잘 이해했기 때문일 수 있습니다. 예를 들어 로켓이 불면, 해석 가능한 모델이 근본 원인을 추적하는 데 도움이 될 수 있습니다.\n",
    "\n",
    "### 소개\n",
    "\n",
    "- 이 장에서는 HMM (Hidden Markov Models)을 소개합니다. HMM은 연구중인 문제에 대한 직관적인 속성을 나타냅니다. \n",
    "- 마르코프 체인과 HMM에 대해 자세히 설명하기 전에 몇 가지 대안 모델을 살펴 보겠습니다. \n",
    "\n",
    "\n",
    "### 6.1 해석 할 수 없는 모델의 예\n",
    "- 해석하기 어려운 블록박스 머신학습 알고리즘의 전형적인 예가 있습니다. \n",
    "- 이미지 분류 작업에서 목표는 각 이미지에 레이블을 지정하는 것입니다. 더 간단히 말하면, 이미지 분류는 객관식 문제로 제기되는 경우가 많습니다. \"나열된 카테고리 중 어느 것이 이미지를 가장 잘 설명합니까?\" 기계학습 전문가는 이 문제를 해결하기 위해 엄청난 발전을 이루었습니다. 오늘날 최고의 이미지 분류기는 특정 데이터 세트의 인간 수준 성능과 일치합니다.\n",
    "- 당신은 많은 매개 변수를 배우게 되는 기계학습 모델의 한 종류 인 CNN에 대한 후반부에서 이 문제를 해결하는 방법을 배우게됩니다. 그러나 CNN의 문제이기도 합니다. 수백만 개가 아니라해도 수천 개의 매개 변수가 각각 무엇을 의미합니까? 이미지 분류 기준에 왜 그렇게 했는지 결정하는 것은 어렵습니다. \n",
    "- 기계 학습은 때로 결론에 도달하는 방법에 대한 통찰력을 밝히지 않고 특정 문제를 해결하는 블랙박스 도구라는 평판을 얻습니다. \n",
    "- 이 장의 목적은 기계학습 영역을 해석 가능한 모델로 공개하는 것입니다. 특히 HMM에 대해 배우고 TensorFlow를 사용하여 이를 구현합니다.\n",
    "\n",
    "\n",
    "### 6.2 마르코프 모델\n",
    "- 안드레이 마르코프 (Andrey Markov)는 러시아 수학자로 임의성이 존재할 때 시스템이 어떻게 변하는지를 연구했습니다.\n",
    "- 대기 중에 가스 입자가 튀는 것을 상상해보십시오. 뉴턴 물리학에 의한 각 입자의 위치 추적은 너무 복잡해지므로 무작위성 도입은 물리적 모델을 조금 단순화하는 데 도움이 됩니다.\n",
    "- 마르코프 (Markov)는 가스입자 주위의 이웃만 모델링한다고 생각하면 무작위 시스템을 단순화하는 데 도움이 된다는 것을 깨달았습니다. 예를 들어, 유럽의 가스 입자가 미국의 입자에 거의 영향을 미치지 않았을 수 있습니다. 수학은 전체 시스템 대신에 근처의 이웃만을 볼 때 단순화됩니다. 이 개념을 이제 Markov 속성이라고 합니다.\n",
    "- 날씨를 모델링하는 것을 고려하십시오. 기상 학자는 날씨를 예측하는 데 도움이 되는 온도계, 기압계 및 풍속계와 관련된 다양한 조건을 평가합니다. \n",
    "- Markov 속성을 사용하여 간단한 모델을 시작할 수있는 방법을 살펴보겠습니다. 첫째로, 우리가 연구하고자 하는 상황이나 주를 식별합니다. 그림 6.1은 흐린 날씨, 비오는 날씨, 햇볕이 잘드는 3 가지 기상 상태를 그래프 형태의 노드로 보여줍니다.\n",
    "\n",
    "![6_2](./img/06_02.png)\n",
    "\n",
    "- 그림 6.2는 화살표가 다음의 미래 상태를 가리키며 노드들 사이에 그려진 방향성 에지로서의 전환을 보여줍니다. \n",
    "- 각 모서리에는 확률을 나타내는 가중치가 있습니다 (예 : 오늘 비가 오면 내일이면 흐려질 확률이 30 %입니다). \n",
    "- 전이확률은 과거 데이터에서 알 수 있지만, 지금은 그들이 우리에게 주어진 것으로 가정합시다.\n",
    "- 세 가지 상태가 있는 경우 전환을 3x3 행렬로 나타낼 수 있습니다. (행 i와 열 j에서) 행렬의 각 요소는 노드 i에서 노드 j까지의 에지와 관련된 확률에 해당합니다. 일반적으로 N 개의 주 (state)가 있다면, 천이 행렬은 NxN 크기가됩니다.\n",
    "- 이 시스템을 마르코프 모델이라고 부릅니다. 시간이 지남에 따라 이전 정의된 전이 확률을 사용하여 상태가 변경됩니다 (그림 6.2 참조). \n",
    "- 그림 6.3은 전이 확률이 주어질 때 상태가 어떻게 변하는지를 시각화하는 또 다른 방법입니다. 흔히 트렐리스 다이어그램이라고하며, 나중에 TensorFlow 알고리즘을 구현하는 데 필수적인 도구로 밝혀졌습니다.\n",
    "\n",
    "![6_3](./img/06_03.png)\n",
    "\n",
    "- 이전 장에서 TensorFlow 코드가 계산을 나타내는 그래프를 작성하는 방법을 살펴 보았습니다. 마르코프 모델의 각 노드를 TensorFlow의 노드로 취급하는 것이 유혹적일 수 있습니다. 그러나 그림 6.2와 6.3이 상태 전이를 멋지게 시각화하더라도 그림 6.4에서 볼 수 있듯이 코드에서이를 구현하는 데 더 효율적인 방법이 있습니다.\n",
    "\n",
    "![6_4](./img/06_04.png)\n",
    "\n",
    "- TensorFlow 그래프의 노드는 Tensors이므로 TensorFlow에서 단순하게 노드로 전환 행렬을 나타낼 수 있습니다. 다음 TensorFlow 노드에서 수학 연산을 적용하여 흥미로운 결과를 얻을 수 있습니다.\n",
    "- 예를 들어 비오는 날보다 화창한 날을 선호한다고 가정하면 매일 관련 점수를 얻게됩니다. 각 상태에 대한 점수를 3x1 행렬로 표현합니다. 그런 다음 TensorFlow tf.matmul(T * s)의 두 행렬을 곱하면 각 상태에서 예상되는 전환 우선 순위를 얻을 수 있습니다.\n",
    "\n",
    "\n",
    "### 6.3 Hidden Markov Model\n",
    "- 이전 섹션에서 정의한 마르코프 모델은 모든 상태를 관찰할 수 있을 때 편리하지만 항상 그런 것은 아닙니다. \n",
    "- 도시의 온도 판독 값에만 접근하는 것을 고려하십시오. 그러면 파생 데이터만 주어지면 날씨를 어떻게 추측 할 수 있습니까?\n",
    "\n",
    "- 비오는 날씨는 대부분 온도 판독 값이 낮지만 화창한 날은 온도 판독 값이 높습니다. \n",
    "- 온도 지식과 전이 확률만으로도 가장 가능성이 높은 날씨에 대한 지능적인 추론을 할 수 있습니다. \n",
    "\n",
    "- 이러한 모델은 숨겨진 마르코프 모델(HMM)이라고 불립니다. 왜냐하면 세계의 실제 상태 (비가 내리는 지 햇살인지 등)가 직접 관찰 할 수 없기 때문입니다. \n",
    "- 이러한 숨겨진 상태는 Markov 모델을 따르며 각 상태는 가능성이 있는 측정 가능한 관찰을 방출합니다. 예를 들어, \"Sunny\"의 숨겨진 상태는 고온 판독 값을 방출하지만 경우에 따라 판독 값이 낮은 경우도 있습니다.\n",
    "\n",
    "- HMM에서 우리는 방출 확률을 정의해야한다. 방출 확률은 일반적으로 방출 행렬이라 불리는 행렬로 표현된다. 행렬의 행 수는 상태 수 (Sunny, Cloudy, Rainy)이며 열 수는 여러 유형의 관측 수 (Hot, Mild, Cold)입니다. 행렬의 각 요소는 방출과 관련된 확률입니다.\n",
    "\n",
    "- ** 방출확률(emission probablity)이라고도 불립니다. 은닉된 상태로부터 관측치가 튀어나올 확률 **\n",
    "\n",
    "![6_5](./img/06_05.png)\n",
    "\n",
    "- HMM은 전이확률, 방출 확률, 초기 확률에 대한 설명입니다. 초기 확률은 사전 지식없이 각 상태가 발생할 확률입니다. 우리가 로스 엔젤레스에서 날씨를 모델링한다면 아마도 \"Sunny\"의 초기 확률은 훨씬 더 클 것입니다. 또는 시애틀에서 날씨를 모델링한다고 가정 해 보겠습니다. 잘 알다시피, \"Rainy\"의 초기 확률을 더 높게 설정할 수 있습니다.\n",
    "\n",
    "- HMM을 사용하면 일련의 관찰을 이해할 수 있습니다. 이 기상 모델링 시나리오에서 우리가 물을 수있는 하나의 질문은 온도 판독 값의 특정 순서를 관찰 할 확률입니까? 우리는 Forward 알고리즘을 사용하여 이 질문에 답할 것입니다.\n",
    "\n",
    "\n",
    "### 6.4 순방향 알고리즘\n",
    "- 순방향 알고리즘은 관측 확률을 계산합니다. 특정 관찰을 야기 할 수있는 많은 순열이 있으므로 모든 가능성을 열거하면 순진한 방법으로 계산할 때 기하 급수적으로 시간이 오래 걸릴 것입니다.\n",
    "\n",
    "- 대신 동적 프로그래밍을 사용하여 문제를 해결할 수 있습니다. 이 문제는 복잡한 문제를 간단한 작은 문제로 분해하고 조회 테이블을 사용하여 결과를 캐시하는 전략입니다. 우리 코드에서는 룩업 테이블을 NumPy 배열로 저장하고 TensorFlow 연산에 피드를 제공하여 업데이트를 계속합니다.\n",
    "\n",
    "```\n",
    "P(3 1 3) = P(3 1 3, cold cold cold)+\n",
    "           P(3 1 3, cold cold hot)+...\n",
    "           P(3 1 3, hot hot hot)\n",
    "\n",
    "```\n",
    "[ratsgo's blog Reference](https://ratsgo.github.io/machine%20learning/2017/03/18/HMMs/)\n",
    "\n",
    "![6_5](./img/forward.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![6_6](./img/06_06.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.debugger import Tracer\n",
    "#https://docs.python.org/3/library/pdb.html#pdb.Pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'strided_slice:0' shape=() dtype=int32>, 1]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tf.shape(tf.constant(np.array([[0.1, 0.4, 0.5], [0.6, 0.3, 0.1]])))[0],1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HMM(object):\n",
    "    def __init__(self, initial_prob, trans_prob, obs_prob):\n",
    "        self.N = np.size(initial_prob)\n",
    "        self.initial_prob = initial_prob\n",
    "        self.trans_prob = trans_prob\n",
    "        self.emission = tf.constant(obs_prob)\n",
    "\n",
    "        assert self.initial_prob.shape == (self.N, 1)\n",
    "        assert self.trans_prob.shape == (self.N, self.N)\n",
    "        assert obs_prob.shape[0] == self.N\n",
    "\n",
    "        self.obs_idx = tf.placeholder(tf.int32)\n",
    "        self.fwd = tf.placeholder(tf.float64)\n",
    "        #Tracer()()\n",
    "\n",
    "    def get_emission(self, obs_idx):\n",
    "        slice_location = [0, obs_idx]\n",
    "        num_rows = tf.shape(self.emission)[0]\n",
    "        slice_shape = [num_rows, 1]\n",
    "        return tf.slice(self.emission, slice_location, slice_shape)\n",
    "\n",
    "    #tf.slice텐서의 특정 부분을 추출합니다. 이 함수는 텐서 input에서 begin 위치에서 시작해 크기 size인 부분을 추출합니다\n",
    "\n",
    "    def forward_init_op(self):\n",
    "        obs_prob = self.get_emission(self.obs_idx)\n",
    "        #print(obs_prob)\n",
    "        fwd = tf.multiply(self.initial_prob, obs_prob) #Returns x * y element-wise\n",
    "        return fwd\n",
    "\n",
    "    def forward_op(self):\n",
    "        transitions = tf.matmul(self.fwd, tf.transpose(self.get_emission(self.obs_idx))) #tf.transpose 전치\n",
    "        weighted_transitions = transitions * self.trans_prob\n",
    "        fwd = tf.reduce_sum(weighted_transitions, 0)\n",
    "        #Tracer()()\n",
    "        return tf.reshape(fwd, tf.shape(self.fwd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_algorithm(sess, hmm, observations):\n",
    "    fwd = sess.run(hmm.forward_init_op(), feed_dict={hmm.obs_idx: observations[0]})\n",
    "    print(fwd, '0') \n",
    "    for t in range(1, len(observations)):\n",
    "        fwd = sess.run(hmm.forward_op(), feed_dict={hmm.obs_idx: observations[t], hmm.fwd: fwd})\n",
    "        print(fwd, t) \n",
    "    prob = sess.run(tf.reduce_sum(fwd))\n",
    "    #Tracer()()\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.06]\n",
      " [ 0.24]] 0\n",
      "[[ 0.0552]\n",
      " [ 0.0486]] 1\n",
      "[[ 0.023232]\n",
      " [ 0.013716]] 2\n",
      "[[ 0.0108744 ]\n",
      " [ 0.00151992]] 3\n",
      "[[ 0.00328802]\n",
      " [ 0.00125228]] 4\n",
      "Probability of observing [0, 1, 1, 2, 1] is 0.004540300799999999\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    initial_prob = np.array([[0.6], [0.4]])\n",
    "    trans_prob = np.array([[0.7, 0.3], [0.4, 0.6]])\n",
    "    obs_prob = np.array([[0.1, 0.4, 0.5], [0.6, 0.3, 0.1]])\n",
    "\n",
    "    hmm = HMM(initial_prob=initial_prob, trans_prob=trans_prob, obs_prob=obs_prob)\n",
    "\n",
    "    observations = [0, 1, 1, 2, 1]\n",
    "    with tf.Session() as sess:\n",
    "        prob = forward_algorithm(sess, hmm, observations)\n",
    "        print('Probability of observing {} is {}'.format(observations, prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#x = np.zeros((3, 5, 2), dtype=np.complex128)\n",
    "#print(x)\n",
    "#x.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 두 개의 TensorFlow 작업을 정의해야 합니다. 첫번째 목록 (forward_init_op)은 한 번만 실행되어 순방향 알고리즘의 캐시를 초기화합니다.\n",
    "- 그리고 다음 op는 각 관찰에서 캐시를 업데이트할 것입니다 (forward_op에서 볼 수 있듯이). 이 코드를 실행하는 것은 종종 순방향 단계 실행이라고 합니다. \n",
    "- forward_op 함수는 입력을 받지 않지만 실제로 세션에 공급해야 하는 자리 표시 자 변수에 따라 달라집니다. 특히, self.fwd와 self.obs_idx는이 함수의 입력 값입니다.\n",
    "\n",
    "- HMM 클래스 밖에서, forward_algorithm에서와 같이 전달 알고리즘을 실행하는 함수를 정의해 보자. 순방향 알고리즘은 각 관찰에 대해 순방향 단계를 실행합니다. 결국, 그것은 최종적으로 관측 확률을 산출합니다.\n",
    "- main 함수에서 초기 확률 벡터, 전이확률 행렬 및 emission probability matrix을 입력하여 HMM 클래스를 설정합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 일반적으로이 세 가지 개념은 다음과 같이 정의됩니다.\n",
    "\n",
    "#### 1. 초기 확률 벡터 (initial probability vector)\n",
    "#### 2. 전이확률 행렬 (Transition probability matrix) \n",
    "#### 3. 방출확률 행렬 (emission probability matrix) \n",
    "\n",
    "- 이 행렬이 주어지면 방금 정의한 순방향 알고리즘을 호출합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Viterbi decode\n",
    "- 비터비 (Viterbi) 디코딩 알고리즘은 일련의 관찰을 통해 숨은 상태의 가장 가능성 있는 시퀀스를 찾습니다. \n",
    "- 순방향 알고리즘과 유사한 캐싱 체계가 필요합니다. 캐시 이름을 viterbi로 지정하겠습니다. HMM 생성자에서 Listing 6.7과 같은 줄을 추가한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initial parameters can be learned on training data\n",
    "# theory reference https://web.stanford.edu/~jurafsky/slp3/8.pdf\n",
    "# code reference https://phvu.net/2013/12/06/sweet-implementation-of-viterbi-in-python/\n",
    "class HMM(object):\n",
    "    def __init__(self, initial_prob, trans_prob, obs_prob):\n",
    "\n",
    "        self.N = np.size(initial_prob)\n",
    "        self.initial_prob = initial_prob\n",
    "        self.trans_prob = trans_prob\n",
    "        self.obs_prob = obs_prob #추가된 것\n",
    "        self.emission = tf.constant(obs_prob)\n",
    "        assert self.initial_prob.shape == (self.N, 1)\n",
    "        assert self.trans_prob.shape == (self.N, self.N)\n",
    "        assert self.obs_prob.shape[0] == self.N\n",
    "        self.obs = tf.placeholder(tf.int32)\n",
    "        self.fwd = tf.placeholder(tf.float64)\n",
    "        self.viterbi = tf.placeholder(tf.float64) #추가된 것\n",
    "\n",
    "    def get_emission(self, obs_idx):\n",
    "        slice_location = [0, obs_idx]\n",
    "        num_rows = tf.shape(self.emission)[0]\n",
    "        slice_shape = [num_rows, 1]\n",
    "        return tf.slice(self.emission, slice_location, slice_shape)\n",
    "\n",
    "    def forward_init_op(self):\n",
    "        obs_prob = self.get_emission(self.obs)\n",
    "        fwd = tf.multiply(self.initial_prob, obs_prob)\n",
    "        return fwd\n",
    "\n",
    "    def forward_op(self):\n",
    "        transitions = tf.matmul(self.fwd, tf.transpose(self.get_emission(self.obs)))\n",
    "        weighted_transitions = transitions * self.trans_prob\n",
    "        fwd = tf.reduce_sum(weighted_transitions, 0)\n",
    "        return tf.reshape(fwd, tf.shape(self.fwd))\n",
    "\n",
    "    def decode_op(self):\n",
    "        transitions = tf.matmul(self.viterbi, tf.transpose(self.get_emission(self.obs)))\n",
    "        weighted_transitions = transitions * self.trans_prob\n",
    "        viterbi = tf.reduce_max(weighted_transitions, 0)\n",
    "        return tf.reshape(viterbi, tf.shape(self.viterbi))\n",
    "\n",
    "    def backpt_op(self):\n",
    "        back_transitions = tf.matmul(self.viterbi, np.ones((1, self.N)))\n",
    "        weighted_back_transitions = back_transitions * self.trans_prob\n",
    "        return tf.argmax(weighted_back_transitions, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_algorithm(sess, hmm, observations):\n",
    "    fwd = sess.run(hmm.forward_init_op(), feed_dict={hmm.obs: observations[0]})\n",
    "    print(fwd, '0') \n",
    "    for t in range(1, len(observations)):\n",
    "        fwd = sess.run(hmm.forward_op(), feed_dict={hmm.obs: observations[t], hmm.fwd: fwd})\n",
    "        print(fwd, t) \n",
    "    prob = sess.run(tf.reduce_sum(fwd))\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 마지막으로, HMM 외부의 비터비 디코딩 함수를 정의한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now, let's compute the Viterbi likelihood of the observed sequence:\n",
    "def viterbi_decode(sess, hmm, observations):\n",
    "    viterbi = sess.run(hmm.forward_init_op(), feed_dict={hmm.obs: observations[0]})\n",
    "    print(viterbi, '0') \n",
    "    backpts = np.ones((hmm.N, len(observations)), 'int32') * -1\n",
    "    print(backpts)\n",
    "    for t in range(1, len(observations)):\n",
    "        viterbi, backpt = sess.run([hmm.decode_op(), hmm.backpt_op()],\n",
    "                                    feed_dict={hmm.obs: observations[t],\n",
    "                                               hmm.viterbi: viterbi})\n",
    "        print(viterbi, t) \n",
    "        backpts[:, t] = backpt\n",
    "        print(backpts)\n",
    "    tokens = [viterbi[:, -1].argmax()]\n",
    "    print(tokens)\n",
    "    for i in range(len(observations) - 1, 0, -1):\n",
    "        tokens.append(backpts[tokens[-1], i])\n",
    "    return tokens[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.3 ]\n",
      " [ 0.04]] 0\n",
      "[[ 0.0904]\n",
      " [ 0.0342]] 1\n",
      "[[ 0.030784]\n",
      " [ 0.014292]] 2\n",
      "[[ 0.00272656]\n",
      " [ 0.01068624]] 3\n",
      "[[ 0.00247324]\n",
      " [ 0.00216891]] 4\n",
      "Probability of observing [0, 1, 1, 2, 1] is 0.0046421488\n",
      "[[ 0.3 ]\n",
      " [ 0.04]] 0\n",
      "[[-1 -1 -1 -1 -1]\n",
      " [-1 -1 -1 -1 -1]]\n",
      "[[ 0.084]\n",
      " [ 0.027]] 1\n",
      "[[-1  0 -1 -1 -1]\n",
      " [-1  0 -1 -1 -1]]\n",
      "[[ 0.02352]\n",
      " [ 0.00756]] 2\n",
      "[[-1  0  0 -1 -1]\n",
      " [-1  0  0 -1 -1]]\n",
      "[[ 0.0016464]\n",
      " [ 0.0042336]] 3\n",
      "[[-1  0  0  0 -1]\n",
      " [-1  0  0  0 -1]]\n",
      "[[ 0.00067738]\n",
      " [ 0.00076205]] 4\n",
      "[[-1  0  0  0  1]\n",
      " [-1  0  0  0  1]]\n",
      "[1]\n",
      "Most likely hidden states are [0, 0, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "#    states = ('Healthy', 'Fever')\n",
    "#     observations = ('normal', 'cold', 'dizzy')\n",
    "#     start_probability = {'Healthy': 0.6, 'Fever': 0.4}\n",
    "#     transition_probability = {\n",
    "#         'Healthy': {'Healthy': 0.7, 'Fever': 0.3},\n",
    "#         'Fever': {'Healthy': 0.4, 'Fever': 0.6}\n",
    "#     }\n",
    "#     emission_probability = {\n",
    "#         'Healthy': {'normal': 0.5, 'cold': 0.4, 'dizzy': 0.1},\n",
    "#         'Fever': {'normal': 0.1, 'cold': 0.3, 'dizzy': 0.6}\n",
    "#     }\n",
    "    initial_prob = np.array([[0.6], [0.4]])\n",
    "    trans_prob = np.array([[0.7, 0.3], [0.4, 0.6]])\n",
    "    obs_prob = np.array([[0.5, 0.4, 0.1], [0.1, 0.3, 0.6]])\n",
    "    hmm = HMM(initial_prob=initial_prob, trans_prob=trans_prob, obs_prob=obs_prob)\n",
    "\n",
    "    observations = [0, 1, 1, 2, 1]\n",
    "    with tf.Session() as sess:\n",
    "        prob = forward_algorithm(sess, hmm, observations)\n",
    "        print('Probability of observing {} is {}'.format(observations, prob))\n",
    "\n",
    "        seq = viterbi_decode(sess, hmm, observations)\n",
    "        print('Most likely hidden states are {}'.format(seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6 숨겨진 마르코프 모델의 사용\n",
    "- 순방향 통과 및 비터비 알고리즘을 구현 했으므로 새로 발견 된 기능에 대한 흥미로운 사용법을 살펴 보겠습니다.\n",
    "\n",
    "#### 6.6.1 비디오 모델링\n",
    "- 그들이 걷는 방법에만 전적으로 의존 할 수 있다고 상상해보십시오. 보행을 기반으로 사람들을 식별하는 것은 매우 멋진 아이디어이지만 먼저 보행을 인식하는 모델이 필요합니다. 보행 상태에 대한 숨겨진 상태의 시퀀스가 (1) 휴지 위치, (2) 오른쪽 발 앞으로, (3) 휴지 위치, (4) 왼발 앞으로, 그리고 마지막으로 (5) 휴지 위치 인 HMM을 고려하십시오. 관찰 된 상태는 비디오 클립에서 가져온 걷기 / 조깅 / 달리기의 실루엣입니다.\n",
    "\n",
    "#### 6.6.2 DNA 모델링\n",
    "- DNA는 일련의 뉴클레오타이드이며, 우리는 점차 그 구조에 대해 더 많이 배우고 있습니다. 긴 DNA 문자열을 이해하는 영리한 방법은 나타나는 순서에 대해 확률을 알면 지역을 모델링하는 것입니다. 비오는 날에 흐린 날이 흔한 것처럼 DNA 서열 (시작 코돈)의 특정 영역이 다른 영역 (중지 코돈)보다 더 일반적 일 수 있습니다.\n",
    "\n",
    "#### 6.6.3 이미지 모델링\n",
    "- 필기 인식에서 우리는 필기 단어의 이미지에서 평문을 검색하는 것을 목표로합니다. 한 가지 방법은 한 번에 하나씩 문자를 분석 한 다음 결과를 연결하는 것입니다. 문자가 HMM을 만드는 순서 - 단어 -로 작성된다는 통찰력을 사용할 수 있습니다. 이전의 성격을 아는 것은 아마도 다음 성향의 가능성을 배제하는 데 도움이 될 수 있습니다. 숨겨진 상태는 일반 텍스트이며 관측치는 개별 문자가 포함 된 잘린 이미지입니다.\n",
    "\n",
    "### 6.7 은닉 마르코프 모델의 적용\n",
    "- 숨겨진 마코프 모델은 숨겨진 상태가 무엇인지, 시간이 지남에 따라 어떻게 변하는 지 직감력이있을 때 가장 효과적입니다. 운좋게도 자연 언어 처리 분야에서 문장의 품사를 태그하는 것은 HMM을 사용하여 다소 해결할 수 있습니다.\n",
    "\n",
    "- 문장 내의 단어들의 순서는 HMM의 관측과 일치한다. 예를 들어, \"Pod bay doors, HAL 열기\"라는 문장에는 6 개의 단어가 있습니다.\n",
    "- 숨겨진 상태는 \"동사\", \"명사\", \"형용사\"등과 같은 품사 (part of of speech)입니다. 이전 예제에서 관찰 된 단어 \"open\"은 \"verb\"의 숨겨진 상태와 일치해야합니다.\n",
    "\n",
    "- 전이 확률은 프로그래머가 설계하거나 데이터를 통해 얻을 수 있습니다. 본질적으로, 그것은 품사 (part-of-speech)의 규칙을 나타냅니다. 예를 들어, 두 개의 동사가 차례로 나타날 확률은 매우 낮아야합니다. 전이 확률을 설정함으로써 알고리즘은 모든 가능성을 무차별 적으로 회피합니다.\n",
    "- 각 단어의 출현 확률은 데이터에서 얻을 수 있습니다. \n",
    "\n",
    "### 6.8 요약\n",
    "- 이제 숨겨진 마르코프 모델을 사용하여 자신의 실험을 설계하는 데 필요한 것이 있습니다. 강력한 도구이기 때문에 자신의 데이터로 사용해 보시기 바랍니다. \n",
    "- 일부 전환과 방출을 미리 정의하고 숨겨진 상태를 복구 할 수 있는지 확인하십시오. 바라기를이 장은 당신을 시작하게 도와 줄 수 있기를 바랍니다.\n",
    "- HMM의 연구는 끊임없이 확장되고 있으며, 많은 새로운 아이디어와 수정이 항상 이루어졌습니다. 이러한 급속한 발전 중에는 다음과 같은 몇 가지 주요 조치가 있습니다.\n",
    "\n",
    "    1. 복잡한 얽혀있는 시스템은 Markov 모델을 사용하여 단순화 할 수 있습니다.\n",
    "    2. 숨겨진 마코프 모델은 대부분의 관측이 숨겨진 상태를 측정하기 때문에 실제 응용 프로그램에서보다 유용합니다.\n",
    "    3. 순방향 통과 및 비터 비 알고리즘은 HMM에서 사용되는 가장 일반적인 알고리즘 중 하나"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
