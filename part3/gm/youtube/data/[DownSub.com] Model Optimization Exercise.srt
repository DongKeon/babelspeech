1
00:00:00,050 --> 00:00:03,014
The next part of this notebook for you to do,

2
00:00:03,015 --> 00:00:08,474
is to actually implement the simultaneous gradient descent that we'll use to train again.

3
00:00:08,474 --> 00:00:10,952
In the previous DCGAN project,

4
00:00:10,952 --> 00:00:12,549
we provided this part for you.

5
00:00:12,550 --> 00:00:14,429
So now you get to learn a little bit more

6
00:00:14,429 --> 00:00:17,670
about some of the nuts and bolts of building the GAN.

7
00:00:17,670 --> 00:00:21,710
First, you'll need to get a list of all the weights and biases for the generator.

8
00:00:21,710 --> 00:00:26,100
And then separately, get a list of all the weights and biases for the discriminator.

9
00:00:26,100 --> 00:00:27,864
Each of those sets of parameters,

10
00:00:27,864 --> 00:00:30,149
those for the generator and those for the discriminator,

11
00:00:30,149 --> 00:00:31,440
will have different updates.

12
00:00:31,440 --> 00:00:34,283
So you need to keep them apart from each other.

13
00:00:34,283 --> 00:00:39,070
Next, you'll use Adam to propose an update for each of the players.

14
00:00:39,070 --> 00:00:43,740
Simultaneously, each of these updates will be called and each update

15
00:00:43,740 --> 00:00:49,469
will minimize one player's cost with respect to that player's parameters.

16
00:00:49,469 --> 00:00:57,549
You should store these update opts as d_train_opt and g_train_opt.

17
00:00:57,549 --> 00:01:02,574
Finally, we include a shrink_lr variable,

18
00:01:02,575 --> 00:01:04,870
where we change the learning rate.

19
00:01:04,870 --> 00:01:06,968
We'll call this at the end of each epoch to make

20
00:01:06,968 --> 00:01:09,854
the learning rate gradually decrease over time.

21
00:01:09,855 --> 00:01:14,131
You do not actually need to implement anything in the GAN class itself.

22
00:01:14,131 --> 00:01:19,703
This just calls the functions that we've implemented earlier to set up the loss function,

23
00:01:19,703 --> 00:01:21,953
set up the optimizer and so on.

24
00:01:21,953 --> 00:01:27,608
And, it's just a place where we glue together a lot of the pieces that we built earlier.

25
00:01:27,608 --> 00:01:30,818
You don't need to implement anything in the view sample's function,

26
00:01:30,819 --> 00:01:34,840
this is just some code that we use to plot the samples and show them to you later on.

27
00:01:34,840 --> 00:01:37,834
You don't need to implement anything in the train function, either.

28
00:01:37,834 --> 00:01:40,430
We'll go ahead and run everything for you in here.

29
00:01:40,430 --> 00:01:44,644
This is the place where we take all the pieces that you built and we glue them together.

30
00:01:44,644 --> 00:01:49,030
I'll walk through a little bit just to explain what's actually going on.

31
00:01:49,030 --> 00:01:50,980
One thing we do is we set up a saver that will

32
00:01:50,980 --> 00:01:54,549
actually write the variables that you learned to a checkpoint.

33
00:01:54,549 --> 00:01:57,635
If you'd like to take your training model and go and do something else with it,

34
00:01:57,635 --> 00:01:59,346
after this notebook is done,

35
00:01:59,346 --> 00:02:00,813
this will allow you to do so.

36
00:02:00,813 --> 00:02:03,010
You could do something like go and see if you can

37
00:02:03,010 --> 00:02:05,588
make adversarial examples for your classifier.

38
00:02:05,588 --> 00:02:11,079
Or, you could use it to transcribe real photos of address numbers in images.

39
00:02:11,080 --> 00:02:16,389
Next we make one copy of Z that we keep the same throughout the training process so,

40
00:02:16,389 --> 00:02:18,699
that every time we plot the samples,

41
00:02:18,699 --> 00:02:23,875
we're using the same Z values and any changes that you see from one epoch to the next,

42
00:02:23,875 --> 00:02:27,750
are due to learning rather than sampling different values of Z.

43
00:02:27,750 --> 00:02:32,883
We make a tensorflow session in order to run this particular experiment.

44
00:02:32,883 --> 00:02:36,668
Then we initialize all of the weights and biases in the network.

45
00:02:36,669 --> 00:02:39,317
We iterate for a predetermined number of epochs,

46
00:02:39,317 --> 00:02:44,408
while you're debugging you might want to change this to a smaller number but at the end,

47
00:02:44,408 --> 00:02:46,834
we give you enough that you can see a nice learning curve.

48
00:02:46,835 --> 00:02:50,500
Overall it should take less than 25 minutes to run the whole thing.

49
00:02:50,500 --> 00:02:52,502
Once it's ready to run,

50
00:02:52,502 --> 00:02:54,109
we print out the epoch number,

51
00:02:54,110 --> 00:02:57,319
so that you can get a little bit of a progress indicator in the notebook,

52
00:02:57,319 --> 00:03:00,646
during this relatively slow part of the notebook,

53
00:03:00,646 --> 00:03:03,799
this cell is where most of the time is going to be spent.

54
00:03:03,800 --> 00:03:05,310
When we compute the training error,

55
00:03:05,310 --> 00:03:07,360
there is a little bit of a subtlety because

56
00:03:07,360 --> 00:03:10,360
different mini batches can be different sizes.

57
00:03:10,360 --> 00:03:13,120
Most of the mini batches are going to be size 128,

58
00:03:13,120 --> 00:03:18,610
but the number of examples in the training set is not divisible by 128.

59
00:03:18,610 --> 00:03:22,014
So, the last batch of every epoch will be a different size,

60
00:03:22,014 --> 00:03:25,718
because of that we actually need to compute the training accuracy,

61
00:03:25,718 --> 00:03:27,989
keeping track of the fact that there

62
00:03:27,990 --> 00:03:30,310
is a different number of examples in each mini batch.

63
00:03:30,310 --> 00:03:31,810
For each step of training,

64
00:03:31,810 --> 00:03:36,705
we grab an X value or Y value and a label mask from the data set.

65
00:03:36,705 --> 00:03:38,935
If you wanted to build a really serious application,

66
00:03:38,935 --> 00:03:43,580
this is one of the few places where this notebook is actually simplifying things.

67
00:03:43,580 --> 00:03:46,025
In reality, you'd want to use a tensorflow Q,

68
00:03:46,025 --> 00:03:49,188
to be able to read this very quickly in an asynchronous fashion.

69
00:03:49,188 --> 00:03:51,849
But for the purpose of this SVHN problem,

70
00:03:51,849 --> 00:03:56,234
it's fine to just iterate over the num pi variable like this.

71
00:03:56,235 --> 00:04:01,574
Next, we generate the random noise for the input to the generator and num pi.

72
00:04:01,574 --> 00:04:03,520
This is one of the other few places where

73
00:04:03,520 --> 00:04:05,348
the notebook has simplified things a little bit,

74
00:04:05,348 --> 00:04:07,478
compared to what you do for a real application.

75
00:04:07,479 --> 00:04:08,883
For a real application,

76
00:04:08,883 --> 00:04:12,316
you can actually just use a tensorflow random number generator

77
00:04:12,317 --> 00:04:15,158
that would happen on GPU rather than NCPU.

78
00:04:15,158 --> 00:04:21,264
We go ahead and we call the d_opt variable that you made as well as the g_opt variable,

79
00:04:21,264 --> 00:04:24,115
both simultaneously to optimize

80
00:04:24,115 --> 00:04:28,134
each player's cost with respect to that player's parameters simultaneously.

81
00:04:28,134 --> 00:04:30,579
We just feed in all of the data that we pulled for

82
00:04:30,579 --> 00:04:35,045
this iteration and run it through those optimization rules.

83
00:04:35,045 --> 00:04:37,194
At the end of the epoch, we shrink the learning rate

84
00:04:37,194 --> 00:04:39,769
so that we'll use a smaller learning rate on the next epoch.

85
00:04:39,769 --> 00:04:42,189
We print out the training accuracy so that you

86
00:04:42,189 --> 00:04:45,485
can get some idea of how well you're fitting the training set.

87
00:04:45,485 --> 00:04:47,819
When you look at the training accuracy values that we print out,

88
00:04:47,819 --> 00:04:50,220
there is two things you should keep in mind;

89
00:04:50,220 --> 00:04:55,180
One is that we're computing the train accuracy only on the label examples,

90
00:04:55,180 --> 00:05:01,479
we ignore the labels that are thrown away using the zeros and the label mask variables.

91
00:05:01,480 --> 00:05:03,370
We do that so that you can see how well you're

92
00:05:03,370 --> 00:05:05,935
fitting the examples where the labels were provided,

93
00:05:05,935 --> 00:05:08,595
and not get distracted by how well you're

94
00:05:08,595 --> 00:05:12,189
magically guessing the labels on the unlabeled training examples.

95
00:05:12,189 --> 00:05:16,120
One other thing to keep in mind about the training accuracy variable that we print out,

96
00:05:16,120 --> 00:05:20,350
is that it's updated on the fly during the learning process.

97
00:05:20,350 --> 00:05:22,899
Every time we load a mini batch for training,

98
00:05:22,899 --> 00:05:26,439
we keep track of how many examples were correctly

99
00:05:26,439 --> 00:05:30,550
classified at the moment that we actually trained on them.

100
00:05:30,550 --> 00:05:33,259
Over the course of a single epoch,

101
00:05:33,259 --> 00:05:35,274
the model will actually improve.

102
00:05:35,274 --> 00:05:39,680
It will do better at the end of the epoch than at the start of the epoch.

103
00:05:39,680 --> 00:05:41,980
This training accuracy variable that we print out,

104
00:05:41,980 --> 00:05:48,110
is an average over the performance throughout the entire time of the epoch.

105
00:05:48,110 --> 00:05:50,365
So if we looked at a snapshot of the model

106
00:05:50,365 --> 00:05:53,595
at the end of the epoch at the time we actually print this number,

107
00:05:53,595 --> 00:05:56,980
it's accuracy at that point in time would be

108
00:05:56,980 --> 00:06:00,230
slightly higher than the accuracy that we print,

109
00:06:00,230 --> 00:06:02,810
just because the accuracy we print is

110
00:06:02,810 --> 00:06:06,213
an average that looks backwards in time a little bit.

111
00:06:06,213 --> 00:06:08,605
That might confuse you if you weren't aware of it,

112
00:06:08,605 --> 00:06:09,920
because sometimes you'll see that

113
00:06:09,920 --> 00:06:13,819
the test accuracy is actually higher than the train accuracy,

114
00:06:13,819 --> 00:06:18,153
that's because the test_accuracy is computed instantaneously and

115
00:06:18,153 --> 00:06:23,300
the train_accuracy is averaged over the course of training on a whole epoch.

116
00:06:23,300 --> 00:06:24,514
After we finish training on epoch,

117
00:06:24,514 --> 00:06:29,149
we iterate through the whole test set and we compute the accuracy on the test set,

118
00:06:29,149 --> 00:06:31,430
just so that we can add that to our learning curve

119
00:06:31,430 --> 00:06:34,310
and see how much we were over fitting over time.

120
00:06:34,310 --> 00:06:37,387
We also spit out some samples that came from the generators,

121
00:06:37,387 --> 00:06:42,338
so that you can see how the samples adapt over time from one epoch to the next.

122
00:06:42,338 --> 00:06:44,389
And at the very end of the epoch,

123
00:06:44,389 --> 00:06:46,370
we store the accuracies that we

124
00:06:46,370 --> 00:00:00,000
computed in these lists that we'll use to make a plot later on.

