1
00:00:00,000 --> 00:00:03,415
All right everybody. I'm excited for this notebook.

2
00:00:03,415 --> 00:00:07,043
This notebook is all about semi supervised learning with GANs.

3
00:00:07,043 --> 00:00:10,254
A lot of the time people ask me what are GANs actually good for.

4
00:00:10,255 --> 00:00:13,050
You can use them to make all kinds of pretty pictures and so on.

5
00:00:13,050 --> 00:00:17,250
But do we actually need pretty pictures to solve different engineering tasks?

6
00:00:17,250 --> 00:00:19,469
And it turns out that they're actually useful

7
00:00:19,469 --> 00:00:22,375
for a lot of other things besides just making pictures.

8
00:00:22,375 --> 00:00:24,539
One of the things that's the most obviously useful to

9
00:00:24,539 --> 00:00:29,000
most people is that you can use GANs for semi supervised learning.

10
00:00:29,000 --> 00:00:31,829
What semi supervised learning means is that we still want to

11
00:00:31,829 --> 00:00:35,000
train a classifier like we used to do with supervised learning.

12
00:00:35,000 --> 00:00:37,761
But when we train a classifier with supervised learning,

13
00:00:37,761 --> 00:00:42,060
our training set had both input examples like images that we're going to

14
00:00:42,060 --> 00:00:47,325
represent with the letter X and output labels that we represent with the letter Y,

15
00:00:47,325 --> 00:00:51,075
where we'd say for example that an image of a cat is a cat,

16
00:00:51,075 --> 00:00:52,469
an image of a dog as a dog,

17
00:00:52,469 --> 00:00:55,890
or an image of a handwritten digit is either 0,

18
00:00:55,890 --> 00:00:58,240
1 or 2 or so on.

19
00:00:58,240 --> 00:01:03,179
What happens if we don't actually have labels for all of our inputs and

20
00:01:03,179 --> 00:01:05,370
we have to actually be able to learn from something that's just

21
00:01:05,370 --> 00:01:08,563
an image and no information about what category it's in?

22
00:01:08,563 --> 00:01:10,108
That's where GANs come in.

23
00:01:10,108 --> 00:01:15,128
Before when we had a classifier that just takes an input X and gives you an output Y,

24
00:01:15,129 --> 00:01:17,599
we had to train it by telling it specifically,

25
00:01:17,599 --> 00:01:20,879
here's an input X and you need to produce

26
00:01:20,879 --> 00:01:24,920
this specific value Y every time you see this input value X.

27
00:01:24,920 --> 00:01:27,629
With GANs, we can take the discriminator and

28
00:01:27,629 --> 00:01:30,030
we can turn the discriminator into a classifier.

29
00:01:30,030 --> 00:01:34,500
For this notebook we're going to look at the Street View House Numbers data set.

30
00:01:34,500 --> 00:01:38,265
And we're going to train a GAN to classify images

31
00:01:38,265 --> 00:01:43,230
of digits that are photos taken by the Street View car.

32
00:01:43,230 --> 00:01:45,599
There are 10 different digits, 0 through 9.

33
00:01:45,599 --> 00:01:47,760
And for the GAN framework we're going to take

34
00:01:47,760 --> 00:01:51,325
the discriminator and turn it into an 11 class classifier.

35
00:01:51,325 --> 00:01:56,834
The first 10 classes are the class of real images of the different digits 0 through 9.

36
00:01:56,834 --> 00:01:59,489
And then the eleventh class is the class of all fake images.

37
00:01:59,489 --> 00:02:01,798
Because this discriminator gets to learn

38
00:02:01,799 --> 00:02:04,769
inside the GAN framework instead of just learning as a classifier,

39
00:02:04,769 --> 00:02:10,455
it has three different sources of information that it can draw from the training process.

40
00:02:10,455 --> 00:02:13,468
The first is it actually gets to look at real images,

41
00:02:13,468 --> 00:02:16,710
and retell it the label for some of those images.

42
00:02:16,710 --> 00:02:19,530
But then we also have real images where we don't have a label.

43
00:02:19,530 --> 00:02:23,250
On those, all that the classifier learns is that they're real.

44
00:02:23,250 --> 00:02:27,210
It should try to maximize the sum of the probabilities of the different real classes.

45
00:02:27,210 --> 00:02:31,180
But it doesn't know which one it should boost for any particular image.

46
00:02:31,180 --> 00:02:35,044
And then finally, it should learn to reject fake images.

47
00:02:35,044 --> 00:02:37,275
So it gets to train on three different things,

48
00:02:37,275 --> 00:02:39,375
labeled images, unlabeled images,

49
00:02:39,375 --> 00:02:41,443
and completely imaginary images.

50
00:02:41,443 --> 00:02:45,089
That means you have a lot more inputs that we can use to train our classifier

51
00:02:45,090 --> 00:02:49,253
than we would normally have if we were restricted to using only labeled data.

52
00:02:49,253 --> 00:02:51,870
Because we get all these extra inputs,

53
00:02:51,870 --> 00:02:56,639
we can get much lower error and much higher accuracy on the test set than

54
00:02:56,639 --> 00:03:02,455
if we had to rely exclusively on inputs that we had been able to obtain labels for.

55
00:03:02,455 --> 00:00:00,000
All right so let's dive in and look at how we actually build this semi supervised GAN.

