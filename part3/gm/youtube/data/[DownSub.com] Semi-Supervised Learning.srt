1
00:00:00,000 --> 00:00:04,295
So far, we have mostly seen how to use Gans for generating images.

2
00:00:04,295 --> 00:00:06,435
While generating images is fun,

3
00:00:06,435 --> 00:00:10,200
and it is connected to several important AI research directions.

4
00:00:10,200 --> 00:00:12,855
It is immediately useful in only a few niche domains,

5
00:00:12,855 --> 00:00:14,935
like making image editing software,

6
00:00:14,935 --> 00:00:19,030
where the goal is actually to produce a nice image at the end of the day.

7
00:00:19,030 --> 00:00:23,925
A much more generally useful application of Gans is semi supervised learning,

8
00:00:23,925 --> 00:00:28,485
where we actually improve the performance of a classifier using a Gan.

9
00:00:28,485 --> 00:00:33,690
Many more current products and services use classification than generation.

10
00:00:33,690 --> 00:00:37,110
So I imagine a lot of you would be much more excited to learn about how to build

11
00:00:37,110 --> 00:00:40,925
a better classifier than about how to generate images.

12
00:00:40,925 --> 00:00:45,900
Object recognition models based on deep learning often achieve superhuman accuracy,

13
00:00:45,900 --> 00:00:47,820
after they have been trained.

14
00:00:47,820 --> 00:00:50,340
Modern deep learning algorithms are not

15
00:00:50,340 --> 00:00:53,795
yet anywhere near human efficiency during learning.

16
00:00:53,795 --> 00:00:56,575
Consider the Street View House numbers data set

17
00:00:56,575 --> 00:00:59,820
used to train address number transcription models.

18
00:00:59,820 --> 00:01:04,610
This data set contains hundreds of thousands of labeled photos of address numbers.

19
00:01:04,610 --> 00:01:06,445
Recall when you learned to read.

20
00:01:06,445 --> 00:01:08,770
Your teacher did not need to take you on a road trip,

21
00:01:08,770 --> 00:01:13,270
to see a whole city's worth of address numbers and tell you what each of them said.

22
00:01:13,270 --> 00:01:17,950
From this point of view, deep learning seems to require a lot more data than people do.

23
00:01:17,950 --> 00:01:19,750
From another point of view,

24
00:01:19,750 --> 00:01:23,530
we could argue that deep learning is not getting a fair chance because

25
00:01:23,530 --> 00:01:29,380
the entire life experience of a deep learning model is only a group of labeled images.

26
00:01:29,380 --> 00:01:33,510
People are able to learn from very few examples provided by a teacher.

27
00:01:33,510 --> 00:01:35,785
But that is probably because people also have

28
00:01:35,785 --> 00:01:39,730
all kinds of sensory experience that does not come with labels.

29
00:01:39,730 --> 00:01:41,365
As we move around the world,

30
00:01:41,365 --> 00:01:43,885
we see objects in several different lighting conditions,

31
00:01:43,885 --> 00:01:46,465
from several different angles and so on.

32
00:01:46,465 --> 00:01:49,730
We do not receive labels for most of our experiences.

33
00:01:49,730 --> 00:01:52,000
We have a lot of experiences that do not resemble

34
00:01:52,000 --> 00:01:56,770
anything that a modern deep learning algorithm gets to see in its training set.

35
00:01:56,770 --> 00:02:00,895
One path to improving the learning efficiency of deep learning models,

36
00:02:00,895 --> 00:02:03,250
is semi supervised learning.

37
00:02:03,250 --> 00:02:05,430
In semi supervised learning,

38
00:02:05,430 --> 00:02:08,335
the model can learn from labeled examples like usual,

39
00:02:08,335 --> 00:02:13,180
but it can also get better at classification by studying unlabeled examples,

40
00:02:13,180 --> 00:02:16,300
even though those examples have no class label.

41
00:02:16,300 --> 00:02:19,180
Usually it is much easier and cheaper to

42
00:02:19,180 --> 00:02:22,690
obtain unlabeled data than to obtain labeled data.

43
00:02:22,690 --> 00:02:25,600
For example, the internet is essentially a free source of

44
00:02:25,600 --> 00:02:30,265
virtually unlimited amounts of unlabeled text, images and videos.

45
00:02:30,265 --> 00:02:33,695
To do semi supervised classification with Gans,

46
00:02:33,695 --> 00:02:36,525
we will need to set up the Gan to work as a classifier.

47
00:02:36,525 --> 00:02:38,245
Gans contain two models,

48
00:02:38,245 --> 00:02:40,360
the generator and the discriminator.

49
00:02:40,360 --> 00:02:45,165
Usually we train both and then throw the discriminator away at the end of training.

50
00:02:45,165 --> 00:02:49,750
We usually only care about using the generator to create samples.

51
00:02:49,750 --> 00:02:51,160
The discriminator is usually of

52
00:02:51,160 --> 00:02:54,640
secondary importance and only used to train the generator.

53
00:02:54,640 --> 00:02:56,800
For semi supervised learning,

54
00:02:56,800 --> 00:02:58,990
we will actually focus on the discriminator,

55
00:02:58,990 --> 00:03:00,550
rather than the generator.

56
00:03:00,550 --> 00:03:03,535
We will extend the discriminator to be our classifier

57
00:03:03,535 --> 00:03:07,375
and use it to classify new data after we are done training it.

58
00:03:07,375 --> 00:03:09,580
We can actually throw away the generator,

59
00:03:09,580 --> 00:03:12,040
unless we also want to generate images.

60
00:03:12,040 --> 00:03:14,485
For semi supervised classification,

61
00:03:14,485 --> 00:03:16,630
the generator is of secondary importance,

62
00:03:16,630 --> 00:03:19,395
used only to train the discriminator.

63
00:03:19,395 --> 00:03:23,910
So far, we have used the discriminator net with one sigmoid output.

64
00:03:23,910 --> 00:03:29,455
That sigmoid output gives us the probability that the output is real rather than fake.

65
00:03:29,455 --> 00:03:32,605
We can turn this into a soft Max with two outputs,

66
00:03:32,605 --> 00:03:37,295
one corresponding to the real class and one corresponding to the fake class.

67
00:03:37,295 --> 00:03:40,460
If we hard code the logits for the fake class to zero,

68
00:03:40,460 --> 00:03:44,980
then the soft Max computes exactly the same probabilities as the sigmoid to used to.

69
00:03:44,980 --> 00:03:48,965
To turn the discriminator into a useful classifier,

70
00:03:48,965 --> 00:03:53,625
we can split the real class into all of the different classes we want to recognize.

71
00:03:53,625 --> 00:03:56,920
For example, to classify ten different SVHN digits,

72
00:03:56,920 --> 00:03:58,660
zero through nine, we can make

73
00:03:58,660 --> 00:04:01,480
a discriminator that has eleven different classes in total.

74
00:04:01,480 --> 00:04:07,025
Real zeros, real ones and so on up to real nines and then one extra class.

75
00:04:07,025 --> 00:04:09,335
The class of all fake images.

76
00:04:09,335 --> 00:04:12,145
Now we can train the model using the sum of two costs.

77
00:04:12,145 --> 00:04:14,110
For the examples that have labels,

78
00:04:14,110 --> 00:04:17,154
we can use the regular supervised cross entropy cost.

79
00:04:17,154 --> 00:04:19,104
For all of the other examples,

80
00:04:19,105 --> 00:04:21,738
and also for fixed samples from the generator,

81
00:04:21,738 --> 00:04:23,530
we can add the Gan cost.

82
00:04:23,530 --> 00:04:26,035
To get the probability that the input is real,

83
00:04:26,035 --> 00:04:28,930
we just sum over the probabilities for all the real classes.

84
00:04:28,930 --> 00:04:32,750
Normal classifiers can learn only unlabeled images.

85
00:04:32,750 --> 00:04:35,185
This new set up can learn unlabeled images,

86
00:04:35,185 --> 00:04:39,550
real unlabeled images and even fake images from the generator.

87
00:04:39,550 --> 00:04:41,710
Altogether, this results in very low error on

88
00:04:41,710 --> 00:04:45,592
the test set because there are so many different sources of information,

89
00:04:45,592 --> 00:04:47,860
even without using many labeled examples.

90
00:04:47,860 --> 00:04:49,495
To get this to work really well,

91
00:04:49,495 --> 00:04:52,390
we need one more trick, called feature matching.

92
00:04:52,390 --> 00:04:56,950
The idea of feature matching is to add a term to the cost function for the generator,

93
00:04:56,950 --> 00:04:59,290
penalizing the mean absolute error between

94
00:04:59,290 --> 00:05:02,050
the average value of some set of features on a training data,

95
00:05:02,050 --> 00:05:05,605
and the average value of that set of features on the generated samples.

96
00:05:05,605 --> 00:05:09,115
The set of features can be any group of hidden units from the discriminator.

97
00:05:09,115 --> 00:05:12,550
In a paper called Improved Techniques for training Gans,

98
00:05:12,550 --> 00:05:16,160
open AI was able to achieve an error rate of less than six percent

99
00:05:16,160 --> 00:05:20,295
on SVHN using only one thousand labeled examples.

100
00:05:20,295 --> 00:05:24,085
For comparison, the best previous semi supervised learning algorithm,

101
00:05:24,085 --> 00:05:26,170
had over sixteen percent error,

102
00:05:26,170 --> 00:05:28,265
nearly three times higher.

103
00:05:28,265 --> 00:05:31,035
Of course, fully supervised algorithms,

104
00:05:31,035 --> 00:05:33,465
using hundreds of thousands of labeled examples,

105
00:05:33,465 --> 00:05:36,150
are able to achieve less than two percent error.

106
00:05:36,150 --> 00:05:39,540
So, semi supervised learning still has some catching up to do compared to

107
00:05:39,540 --> 00:05:43,425
the brute force approach of just gathering tons and tons of labeled data.

108
00:05:43,425 --> 00:05:46,260
Usually, labeled data is the bottleneck that determines

109
00:05:46,260 --> 00:05:49,785
which tasks we are or are not able to solve with machine learning.

110
00:05:49,785 --> 00:05:52,605
Hopefully, using semi supervised Gans,

111
00:05:52,605 --> 00:00:00,000
you will be able to tackle a lot of problems that were not possible before.

