1
00:00:00,000 --> 00:00:01,820
The model_input's method is

2
00:00:01,820 --> 00:00:06,208
some fairly boilerplate code that sets up things like input's real,

3
00:00:06,208 --> 00:00:09,766
where we feed the actual images that come from the data set.

4
00:00:09,766 --> 00:00:16,619
Input Z where we feed the random noise that we use to guide the generator and Y,

5
00:00:16,620 --> 00:00:18,960
where we load the labels that specify

6
00:00:18,960 --> 00:00:22,125
which of the ten digit classes each of the examples belongs to.

7
00:00:22,125 --> 00:00:24,928
And a placeholder for the label mask,

8
00:00:24,928 --> 00:00:27,358
where we specify which of the examples we should

9
00:00:27,359 --> 00:00:30,219
actually allow ourselves to use the label on.

10
00:00:30,219 --> 00:00:33,630
You've already done a notebook on implementing DCGANS.

11
00:00:33,630 --> 00:00:37,520
So, you've already seen how to build a generator and a discriminator and so on.

12
00:00:37,520 --> 00:00:40,259
Because of that, we're not going to ask you to build the generator

13
00:00:40,259 --> 00:00:43,548
for us via chat and or for this semi-supervised learning task.

14
00:00:43,548 --> 00:00:47,295
You can see here, that is a fairly simple DCGAN style generator,

15
00:00:47,295 --> 00:00:53,954
using batch normalization and convolution transpose to start out with a noise vector,

16
00:00:53,954 --> 00:00:57,255
reshape it to a convolution feature map,

17
00:00:57,255 --> 00:01:02,353
and go through several rounds of convolution transpose to get a larger and larger image,

18
00:01:02,353 --> 00:01:05,069
with fewer and fewer channels until eventually,

19
00:01:05,069 --> 00:01:08,188
we output a 32 by 32 image with

20
00:01:08,188 --> 00:01:11,768
three different channels for the colors red, green and blue.

21
00:01:11,769 --> 00:01:14,790
And at the very end of the generator network,

22
00:01:14,790 --> 00:01:17,760
we use a hyperbolic tangent function to

23
00:01:17,760 --> 00:01:21,674
squash the values to negative one and positive one.

24
00:01:21,674 --> 00:01:26,504
The discriminator network is really the most important part of this whole notebook,

25
00:01:26,504 --> 00:01:28,974
because we're doing semi-supervised learning.

26
00:01:28,974 --> 00:01:31,170
Our goal in semi-supervised learning is to make

27
00:01:31,170 --> 00:01:34,590
a really good classifier that generalizes well to the test set,

28
00:01:34,590 --> 00:01:37,239
even though we don't have many examples.

29
00:01:37,239 --> 00:01:39,015
And the discriminator network,

30
00:01:39,015 --> 00:01:41,063
is that model that we're going to train.

31
00:01:41,063 --> 00:01:43,109
It's now a multi-class classifier.

32
00:01:43,109 --> 00:01:45,689
So let's look at the discriminator net in a lot of detail.

33
00:01:45,689 --> 00:01:49,560
We begin with a basic discriminator net that's really

34
00:01:49,560 --> 00:01:54,578
focused on the DCGAN style of discriminator model.

35
00:01:54,578 --> 00:01:57,884
We use convolution and batch normalization.

36
00:01:57,885 --> 00:02:02,120
We don't ever use any Max pooling or average pooling, or anything like that.

37
00:02:02,120 --> 00:02:05,143
Following the recommendation from the DCGAN paper,

38
00:02:05,143 --> 00:02:09,199
we don't use any batch normalization in the first layer of the discriminator.

39
00:02:09,199 --> 00:02:11,580
If we did that, it would be hard to make sure that

40
00:02:11,580 --> 00:02:15,770
the pixels coming out of the generator have the correct mean and standard deviation.

41
00:02:15,770 --> 00:02:17,550
As we go deeper in the network,

42
00:02:17,550 --> 00:02:21,139
we do actually use batch normalization in a lot of places.

43
00:02:21,139 --> 00:02:24,598
One thing you'll see, is that we don't ever use pooling to

44
00:02:24,598 --> 00:02:28,739
reduce the size of the feature maps anywhere in the network.

45
00:02:28,740 --> 00:02:32,125
Instead, we use convolution with a stride of two.

46
00:02:32,125 --> 00:02:34,843
That's one of the recommendations from the DCGAN paper,

47
00:02:34,843 --> 00:02:36,688
that originated in a paper called,

48
00:02:36,688 --> 00:02:38,275
"The All Convolutional Net."

49
00:02:38,275 --> 00:02:41,348
Another important aspect of this discriminator net,

50
00:02:41,348 --> 00:02:44,340
is that we use dropout in several places.

51
00:02:44,340 --> 00:02:48,650
We actually use dropout even on the very first convolutional layer.

52
00:02:48,650 --> 00:02:53,158
But we also use dropout in other layers throughout the network.

53
00:02:53,158 --> 00:02:54,870
The reason that we use dropout a little bit

54
00:02:54,870 --> 00:02:57,405
more frequently here than in some of the other models,

55
00:02:57,405 --> 00:03:00,370
is that dropout is a regularization technique.

56
00:03:00,370 --> 00:03:02,938
It helps to make sure that the test error is not too

57
00:03:02,938 --> 00:03:05,728
much higher than the training error for a classifier.

58
00:03:05,729 --> 00:03:10,783
And, because we're working with only 1,000 labeled examples here,

59
00:03:10,783 --> 00:03:13,888
it's really important to reduce the amount that we over fit,

60
00:03:13,889 --> 00:03:15,986
that's going to be the biggest challenge that we're facing,

61
00:03:15,986 --> 00:03:18,563
so we use lots of dropout all over the place.

62
00:03:18,563 --> 00:03:21,209
We also see that we implement leaky relus everywhere.

63
00:03:21,210 --> 00:03:23,955
We can build these by taking the maximum between

64
00:03:23,955 --> 00:03:27,544
Alpha times the input and the input itself.

65
00:03:27,544 --> 00:03:30,044
If you think about it, whenever the input is positive,

66
00:03:30,044 --> 00:03:33,538
the positive version is going to win the max and get passed through.

67
00:03:33,538 --> 00:03:35,293
Whenever the input is negative,

68
00:03:35,294 --> 00:03:40,093
scaling the input down by Alpha will bring it closer to zero so make it greater,

69
00:03:40,093 --> 00:03:44,388
and that means that the scaled negative input will win the max and get passed through.

70
00:03:44,389 --> 00:03:46,560
So it will have a slope of Alpha,

71
00:03:46,560 --> 00:03:48,750
rather than a slope of one,

72
00:03:48,750 --> 00:03:51,538
in terms of a function of the input.

73
00:03:51,538 --> 00:03:53,968
The reason that we use these leaky relus everywhere,

74
00:03:53,968 --> 00:03:57,823
is that they always have a gradient that gets passed down to the layer below.

75
00:03:57,824 --> 00:04:01,033
That gradient needs to propagate really cleanly through the discriminator,

76
00:04:01,033 --> 00:04:05,138
because the gradient of the discriminator is the learning signal for the generator.

77
00:04:05,139 --> 00:04:10,455
Layer seven of the discriminator is particularly interesting in this architecture.

78
00:04:10,455 --> 00:04:13,258
We're going to use a loss function called feature matching,

79
00:04:13,258 --> 00:04:16,055
that was invented by Tim at OpenAI.

80
00:04:16,055 --> 00:04:18,329
The basic idea of feature matching,

81
00:04:18,329 --> 00:04:21,360
is that we take some features from a hidden layer of

82
00:04:21,360 --> 00:04:25,470
the discriminator and we make sure that the average feature value on

83
00:04:25,470 --> 00:04:28,528
the training data is roughly comparable to

84
00:04:28,528 --> 00:04:32,610
the average feature value on the data generated by the generator network.

85
00:04:32,610 --> 00:04:35,855
Because we're going to take the average of different features,

86
00:04:35,855 --> 00:04:37,319
we want to make sure that

87
00:04:37,319 --> 00:04:39,750
the average feature value can

88
00:04:39,750 --> 00:04:42,915
change when you move from one kind of data to the other kind of data.

89
00:04:42,915 --> 00:04:46,454
Because of that, we don't use batch normalization on this layer.

90
00:04:46,454 --> 00:04:51,194
Batch normalization subtracts off the mean of every feature value,

91
00:04:51,194 --> 00:04:57,120
and then adds on an offset parameter that it learns essentially a bias parameter.

92
00:04:57,120 --> 00:05:00,052
Because batch normalization goes ahead and sets

93
00:05:00,052 --> 00:05:03,137
the means to be exactly equal to its bias parameter,

94
00:05:03,137 --> 00:05:07,709
that means that these feature values would all get set to just have that particular mean.

95
00:05:07,709 --> 00:05:10,214
And then the feature matching loss,

96
00:05:10,214 --> 00:05:13,568
where we look at the difference in means of the features in the two different settings,

97
00:05:13,569 --> 00:05:17,105
we're not actually going to be able to find any difference at all.

98
00:05:17,105 --> 00:05:21,555
So when we get to this particular layer that we're going to use as those features,

99
00:05:21,555 --> 00:05:22,875
we can't use batched norm,

100
00:05:22,875 --> 00:05:25,459
or it will throw out our learning signal.

101
00:05:25,459 --> 00:05:27,704
Tim got around this by using weight norm,

102
00:05:27,704 --> 00:05:32,069
it took a little bit of careful coordination of the design of

103
00:05:32,069 --> 00:05:36,778
the initialization of all the weights to make that weight norm network work really well.

104
00:05:36,778 --> 00:05:39,600
To avoid the code complexity that we

105
00:05:39,600 --> 00:05:42,793
need for the data dependent initialization that Tim used,

106
00:05:42,793 --> 00:05:47,384
we're going to simplify things a little bit by using batch normalization in most places.

107
00:05:47,384 --> 00:05:49,410
And then just not

108
00:05:49,410 --> 00:05:55,074
using any kind of weight normalization or batch normalization or anything like that here.

109
00:05:55,074 --> 00:05:59,535
But, if you'd like to learn about weight normalization and how that works,

110
00:05:59,535 --> 00:06:02,629
you should definitely go read the weight normalization paper.

111
00:06:02,629 --> 00:06:06,675
Another technique that works really well for convolutional classifiers,

112
00:06:06,675 --> 00:06:08,535
is global average pooling.

113
00:06:08,535 --> 00:06:10,680
In global average pooling,

114
00:06:10,680 --> 00:06:12,875
for every feature map,

115
00:06:12,875 --> 00:06:15,028
we take the average over all of

116
00:06:15,028 --> 00:06:20,490
the spatial domain of the feature map and pass through just one value.

117
00:06:20,490 --> 00:06:22,649
So, before we get to this layer,

118
00:06:22,649 --> 00:06:26,355
we have a tensor that is batch size by height,

119
00:06:26,355 --> 00:06:28,980
by width, by number of channels.

120
00:06:28,980 --> 00:06:32,430
And we're going to take the average over the special dormain and

121
00:06:32,430 --> 00:06:36,685
return just a tensor that is batch size by number of channels.

122
00:06:36,685 --> 00:06:40,170
And that will give us a lot of invariants to translation,

123
00:06:40,170 --> 00:06:43,463
if a digit appears in the image in several different locations,

124
00:06:43,463 --> 00:06:45,555
we'll throw that location information out,

125
00:06:45,555 --> 00:06:48,519
because we've averaged over all the different spatial positions.

126
00:06:48,519 --> 00:06:53,115
Now we get to the point where you will implement the actual classifier.

127
00:06:53,115 --> 00:06:55,649
You should add some kind of fully connected layer

128
00:06:55,649 --> 00:06:59,910
here that outputs a distribution over the different classes.

129
00:06:59,910 --> 00:07:02,355
Now we're at the most complicated part of the notebook,

130
00:07:02,355 --> 00:07:05,879
where we actually need to implement the output of

131
00:07:05,879 --> 00:07:10,639
the discriminator in the form that we will use for semi-supervised GAN training.

132
00:07:10,639 --> 00:07:14,550
We already have the logits associated with all of the different classes,

133
00:07:14,550 --> 00:07:17,910
where the logits are just the values that drive

134
00:07:17,910 --> 00:07:19,709
the softmax or the inputs that

135
00:07:19,709 --> 00:07:23,180
the softmax gets normalized to form a distribution over the classes.

136
00:07:23,180 --> 00:07:25,814
But to train this classifier as a GAN,

137
00:07:25,814 --> 00:07:30,790
we actually need to have a probability that the input is real, rather than fake.

138
00:07:30,790 --> 00:07:34,259
The softmax gives us the probabilities of all the different real classes and it

139
00:07:34,259 --> 00:07:38,459
also gives us a probability value for the fake class.

140
00:07:38,459 --> 00:07:40,889
We know that the probability of the input being real,

141
00:07:40,889 --> 00:07:44,884
is the sum over all the probabilities of the real classes.

142
00:07:44,884 --> 00:07:46,095
What we want to do now,

143
00:07:46,095 --> 00:07:50,764
is find a value of the logits that we could plug into a sigmoid.

144
00:07:50,764 --> 00:07:54,300
If we wanted to use that sigmoid as a traditional GAN discriminator,

145
00:07:54,300 --> 00:07:56,925
if we want the output of the sigmoid to be the probability

146
00:07:56,925 --> 00:08:00,170
that the input is real rather than fake.

147
00:08:00,170 --> 00:08:03,375
So we've got an expression for the probability we want,

148
00:08:03,375 --> 00:08:05,204
in terms of softmax logits,

149
00:08:05,204 --> 00:08:10,963
we've got to figure out how to rearrange that to be an expression of sigmoid logits.

150
00:08:10,963 --> 00:08:14,490
So you have to do a bit of algebra to solve this part of the notebook.

151
00:08:14,490 --> 00:08:16,290
There's one other trick that's going to be really

152
00:08:16,290 --> 00:08:18,463
important to make this notebook work well,

153
00:08:18,463 --> 00:08:21,509
once you've done your algebra and you've got

154
00:08:21,509 --> 00:08:25,259
an expression for what the logits to the discriminator's sigmoid should be,

155
00:08:25,259 --> 00:08:27,509
you'll need to actually figure out how to implement

156
00:08:27,509 --> 00:08:31,970
the expression for those logits in a numerically stable way.

157
00:08:31,970 --> 00:08:35,960
Your expression should have a log-sum-exp in it somewhere.

158
00:08:35,960 --> 00:08:41,524
And it turns out that log-sum-exp can run into a lot of different problems.

159
00:08:41,524 --> 00:08:44,970
If you should think about what happens if you feed

160
00:08:44,970 --> 00:08:47,428
a very large value into

161
00:08:47,428 --> 00:08:52,334
the log-sum-exp function somewhere and what might happen when you do that.

162
00:08:52,335 --> 00:08:55,769
You should also think about what might happen if you feed nothing but

163
00:08:55,769 --> 00:08:58,558
very negative values for

164
00:08:58,558 --> 00:09:02,219
all the different entries in the vector that you're going to pass to the log-sum-exp.

165
00:09:02,220 --> 00:09:04,139
If either of those things happens,

166
00:09:04,139 --> 00:09:07,860
the naive implementation of log-sum-exp can go wrong.

167
00:09:07,860 --> 00:09:09,153
In the code comments here,

168
00:09:09,153 --> 00:09:12,509
we've explained that there is a trick where you

169
00:09:12,509 --> 00:09:16,649
can subtract the max off of the vector that you

170
00:09:16,649 --> 00:09:24,879
want to compute the log-sum-exp of and move that max and add it outside the log-sum-exp.

171
00:09:24,879 --> 00:09:26,399
If you look at the algebra,

172
00:09:26,399 --> 00:09:28,918
the two forms of the log-sum-exp,

173
00:09:28,918 --> 00:09:32,603
with the max pulled outside and with the max left alone,

174
00:09:32,604 --> 00:09:35,644
those are analytically equivalent to each other.

175
00:09:35,644 --> 00:09:39,065
But, if you think about the two different cases we highlighted,

176
00:09:39,065 --> 00:09:41,796
where some of the values are very extreme,

177
00:09:41,796 --> 00:09:47,070
you'll see that one of them could have numerical problems and the other one avoids a lot

178
00:09:47,070 --> 00:09:49,320
of these numerical problems and using

179
00:09:49,320 --> 00:00:00,000
the stable version will make your classifier perform really a lot better.

