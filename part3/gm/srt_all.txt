So far, we have mostly seen how to use Gans for generating images. While generating images is fun, and it is connected to several important AI research directions. It is immediately useful in only a few niche domains, like making image editing software, where the goal is actually to produce a nice image at the end of the day. A much more generally useful application of Gans is semi supervised learning, where we actually improve the performance of a classifier using a Gan. Many more current products and services use classification than generation. So I imagine a lot of you would be much more excited to learn about how to build a better classifier than about how to generate images. Object recognition models based on deep learning often achieve superhuman accuracy, after they have been trained. Modern deep learning algorithms are not yet anywhere near human efficiency during learning. Consider the Street View House numbers data set used to train address number transcription models. This data set contains hundreds of thousands of labeled photos of address numbers. Recall when you learned to read. Your teacher did not need to take you on a road trip, to see a whole city's worth of address numbers and tell you what each of them said. From this point of view, deep learning seems to require a lot more data than people do. From another point of view, we could argue that deep learning is not getting a fair chance because the entire life experience of a deep learning model is only a group of labeled images. People are able to learn from very few examples provided by a teacher. But that is probably because people also have all kinds of sensory experience that does not come with labels. As we move around the world, we see objects in several different lighting conditions, from several different angles and so on. We do not receive labels for most of our experiences. We have a lot of experiences that do not resemble anything that a modern deep learning algorithm gets to see in its training set. One path to improving the learning efficiency of deep learning models, is semi supervised learning. In semi supervised learning, the model can learn from labeled examples like usual, but it can also get better at classification by studying unlabeled examples, even though those examples have no class label. Usually it is much easier and cheaper to obtain unlabeled data than to obtain labeled data. For example, the internet is essentially a free source of virtually unlimited amounts of unlabeled text, images and videos. To do semi supervised classification with Gans, we will need to set up the Gan to work as a classifier. Gans contain two models, the generator and the discriminator. Usually we train both and then throw the discriminator away at the end of training. We usually only care about using the generator to create samples. The discriminator is usually of secondary importance and only used to train the generator. For semi supervised learning, we will actually focus on the discriminator, rather than the generator. We will extend the discriminator to be our classifier and use it to classify new data after we are done training it. We can actually throw away the generator, unless we also want to generate images. For semi supervised classification, the generator is of secondary importance, used only to train the discriminator. So far, we have used the discriminator net with one sigmoid output. That sigmoid output gives us the probability that the output is real rather than fake. We can turn this into a soft Max with two outputs, one corresponding to the real class and one corresponding to the fake class. If we hard code the logits for the fake class to zero, then the soft Max computes exactly the same probabilities as the sigmoid to used to. To turn the discriminator into a useful classifier, we can split the real class into all of the different classes we want to recognize. For example, to classify ten different SVHN digits, zero through nine, we can make a discriminator that has eleven different classes in total. Real zeros, real ones and so on up to real nines and then one extra class. The class of all fake images. Now we can train the model using the sum of two costs. For the examples that have labels, we can use the regular supervised cross entropy cost. For all of the other examples, and also for fixed samples from the generator, we can add the Gan cost. To get the probability that the input is real, we just sum over the probabilities for all the real classes. Normal classifiers can learn only unlabeled images. This new set up can learn unlabeled images, real unlabeled images and even fake images from the generator. Altogether, this results in very low error on the test set because there are so many different sources of information, even without using many labeled examples. To get this to work really well, we need one more trick, called feature matching. The idea of feature matching is to add a term to the cost function for the generator, penalizing the mean absolute error between the average value of some set of features on a training data, and the average value of that set of features on the generated samples. The set of features can be any group of hidden units from the discriminator. In a paper called Improved Techniques for training Gans, open AI was able to achieve an error rate of less than six percent on SVHN using only one thousand labeled examples. For comparison, the best previous semi supervised learning algorithm, had over sixteen percent error, nearly three times higher. Of course, fully supervised algorithms, using hundreds of thousands of labeled examples, are able to achieve less than two percent error. So, semi supervised learning still has some catching up to do compared to the brute force approach of just gathering tons and tons of labeled data. Usually, labeled data is the bottleneck that determines which tasks we are or are not able to solve with machine learning. Hopefully, using semi supervised Gans, you will be able to tackle a lot of problems that were not possible before. All right everybody. I'm excited for this notebook. This notebook is all about semi supervised learning with GANs. A lot of the time people ask me what are GANs actually good for. You can use them to make all kinds of pretty pictures and so on. But do we actually need pretty pictures to solve different engineering tasks? And it turns out that they're actually useful for a lot of other things besides just making pictures. One of the things that's the most obviously useful to most people is that you can use GANs for semi supervised learning. What semi supervised learning means is that we still want to train a classifier like we used to do with supervised learning. But when we train a classifier with supervised learning, our training set had both input examples like images that we're going to represent with the letter X and output labels that we represent with the letter Y, where we'd say for example that an image of a cat is a cat, an image of a dog as a dog, or an image of a handwritten digit is either 0, 1 or 2 or so on. What happens if we don't actually have labels for all of our inputs and we have to actually be able to learn from something that's just an image and no information about what category it's in? That's where GANs come in. Before when we had a classifier that just takes an input X and gives you an output Y, we had to train it by telling it specifically, here's an input X and you need to produce this specific value Y every time you see this input value X. With GANs, we can take the discriminator and we can turn the discriminator into a classifier. For this notebook we're going to look at the Street View House Numbers data set. And we're going to train a GAN to classify images of digits that are photos taken by the Street View car. There are 10 different digits, 0 through 9. And for the GAN framework we're going to take the discriminator and turn it into an 11 class classifier. The first 10 classes are the class of real images of the different digits 0 through 9. And then the eleventh class is the class of all fake images. Because this discriminator gets to learn inside the GAN framework instead of just learning as a classifier, it has three different sources of information that it can draw from the training process. The first is it actually gets to look at real images, and retell it the label for some of those images. But then we also have real images where we don't have a label. On those, all that the classifier learns is that they're real. It should try to maximize the sum of the probabilities of the different real classes. But it doesn't know which one it should boost for any particular image. And then finally, it should learn to reject fake images. So it gets to train on three different things, labeled images, unlabeled images, and completely imaginary images. That means you have a lot more inputs that we can use to train our classifier than we would normally have if we were restricted to using only labeled data. Because we get all these extra inputs, we can get much lower error and much higher accuracy on the test set than if we had to rely exclusively on inputs that we had been able to obtain labels for. All right so let's dive in and look at how we actually build this semi supervised GAN. All right. Now we can dive in and start looking at how we actually build this semi-supervised GAN. At the start of this notebook we have a lot of boilerplate code. We go through some standard imports that make sure we have things like tensorflow and numpy. We set up a directory to make sure that we have a place to put all of our data. And, we have a cell that will go ahead and download the data for you. And, just show up progress bars that downloads it. Next, we have some code that actually sets up the data set itself. Will actually, load the dataset out of the downloaded file, will re-scale the features to the range negative one to positive one, will plot a few of the examples so that you can look at them and see what kind of data you're training on. The dataset class is the first place that's really interesting to you for this notebook. Where, you don't actually need to implement anything here but you need to understand carefully what's going on. We have this dataset class that represents both a training set, the validation set and the test set. For this notebook, we're going to be badly behaved and just disregard the validation set. When you make a real product or research project, you should use the validation set to guide your development efforts and help you choose your hyper parameters and so on. Here, just to make this exercise go a bit faster, we're going to cut to the chase and just plot the tests set accuracy. So, you are going to see exactly what you've accomplished in half an hour or less of time running on the GPU rather than how it separately calculate the validation set. The issue that's most important for you to understand is that Street View House Number dataset is essentially fully labeled or at least the portion of it that we're using for this network is fully labeled. But, we were to learn about semi-supervised learning. For semi-supervised learning you don't actually have access to all the labels. So, what we're going to do is we're going to pretend that we don't have all these labels. Specifically, we're going to use this label mask variable in the dataset class to keep track of which labels we're going to pretend we have and which labels are going to pretend we don't have. We're going to restrict ourselves to train with only 1000 labeled examples. That's a very tiny fraction of the actual number of examples that we have available and labeled. But we're going to see that we're able to learn from unlabeled examples, if we ignore their label and just pretend that those weren't available. The main function that you'll use to access the data set class is this batches method. You just need to specify the batch size that you want and it will yield an iterable set of batches that you can just go straight through. You use the which set argument to request either the training set or the test set or the validation set that we don't actually use. We've already set up all of the calls for this method for you but it's useful to you to understand what it's doing. One of the wrinkles of this method is that depending on whether we are looking at the training set or the test set, we will either return or not return a set of label masks that tell you whether you can use a particular label for training or not. In the case of the training set, we will actually include these label masks. In all the other cases, we will not include those label masks. Because, when we evaluate the accuracy on the test set, we actually want to check the accuracy for every single example. So there's no point in having a label mask. The model_input's method is some fairly boilerplate code that sets up things like input's real, where we feed the actual images that come from the data set. Input Z where we feed the random noise that we use to guide the generator and Y, where we load the labels that specify which of the ten digit classes each of the examples belongs to. And a placeholder for the label mask, where we specify which of the examples we should actually allow ourselves to use the label on. You've already done a notebook on implementing DCGANS. So, you've already seen how to build a generator and a discriminator and so on. Because of that, we're not going to ask you to build the generator for us via chat and or for this semi-supervised learning task. You can see here, that is a fairly simple DCGAN style generator, using batch normalization and convolution transpose to start out with a noise vector, reshape it to a convolution feature map, and go through several rounds of convolution transpose to get a larger and larger image, with fewer and fewer channels until eventually, we output a 32 by 32 image with three different channels for the colors red, green and blue. And at the very end of the generator network, we use a hyperbolic tangent function to squash the values to negative one and positive one. The discriminator network is really the most important part of this whole notebook, because we're doing semi-supervised learning. Our goal in semi-supervised learning is to make a really good classifier that generalizes well to the test set, even though we don't have many examples. And the discriminator network, is that model that we're going to train. It's now a multi-class classifier. So let's look at the discriminator net in a lot of detail. We begin with a basic discriminator net that's really focused on the DCGAN style of discriminator model. We use convolution and batch normalization. We don't ever use any Max pooling or average pooling, or anything like that. Following the recommendation from the DCGAN paper, we don't use any batch normalization in the first layer of the discriminator. If we did that, it would be hard to make sure that the pixels coming out of the generator have the correct mean and standard deviation. As we go deeper in the network, we do actually use batch normalization in a lot of places. One thing you'll see, is that we don't ever use pooling to reduce the size of the feature maps anywhere in the network. Instead, we use convolution with a stride of two. That's one of the recommendations from the DCGAN paper, that originated in a paper called, "The All Convolutional Net." Another important aspect of this discriminator net, is that we use dropout in several places. We actually use dropout even on the very first convolutional layer. But we also use dropout in other layers throughout the network. The reason that we use dropout a little bit more frequently here than in some of the other models, is that dropout is a regularization technique. It helps to make sure that the test error is not too much higher than the training error for a classifier. And, because we're working with only 1,000 labeled examples here, it's really important to reduce the amount that we over fit, that's going to be the biggest challenge that we're facing, so we use lots of dropout all over the place. We also see that we implement leaky relus everywhere. We can build these by taking the maximum between Alpha times the input and the input itself. If you think about it, whenever the input is positive, the positive version is going to win the max and get passed through. Whenever the input is negative, scaling the input down by Alpha will bring it closer to zero so make it greater, and that means that the scaled negative input will win the max and get passed through. So it will have a slope of Alpha, rather than a slope of one, in terms of a function of the input. The reason that we use these leaky relus everywhere, is that they always have a gradient that gets passed down to the layer below. That gradient needs to propagate really cleanly through the discriminator, because the gradient of the discriminator is the learning signal for the generator. Layer seven of the discriminator is particularly interesting in this architecture. We're going to use a loss function called feature matching, that was invented by Tim at OpenAI. The basic idea of feature matching, is that we take some features from a hidden layer of the discriminator and we make sure that the average feature value on the training data is roughly comparable to the average feature value on the data generated by the generator network. Because we're going to take the average of different features, we want to make sure that the average feature value can change when you move from one kind of data to the other kind of data. Because of that, we don't use batch normalization on this layer. Batch normalization subtracts off the mean of every feature value, and then adds on an offset parameter that it learns essentially a bias parameter. Because batch normalization goes ahead and sets the means to be exactly equal to its bias parameter, that means that these feature values would all get set to just have that particular mean. And then the feature matching loss, where we look at the difference in means of the features in the two different settings, we're not actually going to be able to find any difference at all. So when we get to this particular layer that we're going to use as those features, we can't use batched norm, or it will throw out our learning signal. Tim got around this by using weight norm, it took a little bit of careful coordination of the design of the initialization of all the weights to make that weight norm network work really well. To avoid the code complexity that we need for the data dependent initialization that Tim used, we're going to simplify things a little bit by using batch normalization in most places. And then just not using any kind of weight normalization or batch normalization or anything like that here. But, if you'd like to learn about weight normalization and how that works, you should definitely go read the weight normalization paper. Another technique that works really well for convolutional classifiers, is global average pooling. In global average pooling, for every feature map, we take the average over all of the spatial domain of the feature map and pass through just one value. So, before we get to this layer, we have a tensor that is batch size by height, by width, by number of channels. And we're going to take the average over the special dormain and return just a tensor that is batch size by number of channels. And that will give us a lot of invariants to translation, if a digit appears in the image in several different locations, we'll throw that location information out, because we've averaged over all the different spatial positions. Now we get to the point where you will implement the actual classifier. You should add some kind of fully connected layer here that outputs a distribution over the different classes. Now we're at the most complicated part of the notebook, where we actually need to implement the output of the discriminator in the form that we will use for semi-supervised GAN training. We already have the logits associated with all of the different classes, where the logits are just the values that drive the softmax or the inputs that the softmax gets normalized to form a distribution over the classes. But to train this classifier as a GAN, we actually need to have a probability that the input is real, rather than fake. The softmax gives us the probabilities of all the different real classes and it also gives us a probability value for the fake class. We know that the probability of the input being real, is the sum over all the probabilities of the real classes. What we want to do now, is find a value of the logits that we could plug into a sigmoid. If we wanted to use that sigmoid as a traditional GAN discriminator, if we want the output of the sigmoid to be the probability that the input is real rather than fake. So we've got an expression for the probability we want, in terms of softmax logits, we've got to figure out how to rearrange that to be an expression of sigmoid logits. So you have to do a bit of algebra to solve this part of the notebook. There's one other trick that's going to be really important to make this notebook work well, once you've done your algebra and you've got an expression for what the logits to the discriminator's sigmoid should be, you'll need to actually figure out how to implement the expression for those logits in a numerically stable way. Your expression should have a log-sum-exp in it somewhere. And it turns out that log-sum-exp can run into a lot of different problems. If you should think about what happens if you feed a very large value into the log-sum-exp function somewhere and what might happen when you do that. You should also think about what might happen if you feed nothing but very negative values for all the different entries in the vector that you're going to pass to the log-sum-exp. If either of those things happens, the naive implementation of log-sum-exp can go wrong. In the code comments here, we've explained that there is a trick where you can subtract the max off of the vector that you want to compute the log-sum-exp of and move that max and add it outside the log-sum-exp. If you look at the algebra, the two forms of the log-sum-exp, with the max pulled outside and with the max left alone, those are analytically equivalent to each other. But, if you think about the two different cases we highlighted, where some of the values are very extreme, you'll see that one of them could have numerical problems and the other one avoids a lot of these numerical problems and using the stable version will make your classifier perform really a lot better. Now that we've defined both our generator model and our discriminator model, it's time to define the loss functions, that will actually tell them what they should do when they start learning. To make the notebook a little bit more convenient, we've set up these size multipliers for you. You've got one for the generator and one for the discriminator. They're just numbers that we multiply by some other hard-coded number to decide the size of each layer on both the generator and the discriminator. If you set these to small values like two or four, then you won't get very good accuracy, but the notebook should run a little bit faster. So you can use those set to small values during the debugging stage, and then set them back to 32 and 64 when you want to actually reproduce the accuracy values that we tell you you ought to be able to get. We've actually gone ahead and run the generator and the discriminator for you. But we should look at these quickly so that you know what we're doing on your behalf. And you can do it again yourself, when you do this in a real project. First off, we run the generator. We call it on the input placeholder that we set up where we'll feed random noise later on as the input to the generator. By running the generator, we get some samples that we can use as input to the discriminator later on. Next we run the discriminator on the real data. We can unpack all the outputs to the discriminator to get things like the probability that the input is real, the logits for the softmax that tells us which of the classes is present in the real data, and we can also get the logits for the sigmoid that tells us whether the input is real or fake. Finally, we also pull out the features that we are going to use from one of the last hidden layers of the discriminator, and we're going to use those for the feature matching loss later on. Next, you run the discriminator again. This time, we apply it to the samples rather than to the real data. There's a few different important things to look at here. The first is that we're now going to enable reuse for the variable scope. This is a tensorflow feature where we don't need to actually allocate the variables that define the weights and biases of the model again. When we call the discriminator function for a second time, it's going to ask for the same variable names as it asked for before. And so by consulting the variable scope, we can recover the variables that we already made in the first call to the discriminator, and that will make sure that we train one discriminator with one set of weights, instead of training one discriminator that looks at the real data and one discriminator that looks at the fake samples. The other thing we should think about here that's a little bit funny is that there's batch normalization inside the discriminator. In the first case, we call the discriminator on the real data and compute all of its batch normalization statistics on the real data. So we compute the mean of all the features on the real data, we subtract that off, we compute the standard deviation of the features on the real data, and we divide by that to get a standard deviation of 1. And then we make a separate call to the discriminator, where we run it again this time purely on samples from the generator, and recompute different values of all those batch normalization statistics. So in a way, we're actually running two different functions here. One function that's normalized using real statistics. One function that's normalized using fake statistics. This seems very counter-intuitive, but the authors of the DCGAN paper found that this works really well. If you look at the paper, "Improved techniques for training GANs," we come up with a few alternatives that are a little bit less weird, but all of them work about the same in practice, even though some of them are more or less weird or more or less difficult to implement. The next task for you is to implement the d_loss variable. This variable needs to end up containing the loss for the discriminator. We've said earlier that we're studying semi-supervised learning in this notebook, and this is where the semi-supervised part happens. We're going to add together two different loss functions. One that's unsupervised and one that's supervised. And the combination of those two losses will make the learning algorithm semi-supervised. The first loss is the unsupervised loss. The loss for the GAN problem where we want to tell the difference between real data and fake samples that came from the generator. We're going to do that by taking the logit for the sigmoid for the discriminator. This logit gives us the unnormalized log probability that the data is real rather than fake. And we're going to set up a cross entropy loss for that binary classification problem. Next, the second loss that we're going to add onto this first one, is the supervised loss. We're going to use a multiclass cross entropy loss to make sure that the softmax over classes gets reshaped to output the right distribution over classes and choose the correct class for each example. When you implement the supervised loss, it's very important to remember to use the label mask variable that we pass in. The label mask variable is a vector that has length batch size. So it has one entry for every example in the mini-batch. Every entry of the label mask variable is either a 0 or 1. If it's zero, it, means that you should ignore the label for that example. We have all the labels. But in order to study semi-supervised learning performance, we need to pretend that some of them are missing. Later on, you could try experimenting with ignoring this label mask variable and seeing if you can actually obtain a higher accuracy. But when you're working in the main semi-supervised learning part, you should be sure that you don't actually use any of the labels that we're asking you to mask out. After you've added up both the costs for the discriminator, it's time to move on and build the cost for the generator. Earlier when we made models like DCGANs, we saw different losses for the generator than we're going to use now. Here we're going to use a loss that Tim at OpenAI invented specifically for semi-supervised learning. Instead of trying to make the classifier make a mistake, the generator is actually going to try to output features that are on average similar to the features that are found by applying the real data to the discriminator. So, up above, we pulled out the features from the last hidden layer of the discriminator. We want to take their average and make sure that the absolute value of the difference between the average features on the data and the average features on the samples is as small as possible, and that will be the only loss that we use for the generator. That loss tends not to work quite as well in terms of producing samples that look realistic, but it works really well for semi-supervised learning. We don't know for sure why it works better for a semi-supervised learning, that's mostly an empirical finding. One thing we can say is that it seems a little bit more like this loss should cause stable learning than some of the other losses that we use, because it gives a specific target for where the generator ought to go. Another thing we can see that might explain why these samples don't look quite as good, is that it doesn't say exactly how every single example should change individually. It just says what the average value of the features in the mini-batch should be. But you can see how if you use more and more features and more and more data, this is just a kind of moment matching in an adversarially learned space. So, it should eventually force recovery of the correct data distribution. Any loss in sample quality is just due to the fact that we're using finite sizes for everything. We finish this section with a little bit of boilerplate code that just takes the argmax across the mini-batch of logits, in order to tell which class is being chosen by the classifier for each entry in the mini-batch. We keep track of how many of those predictions are correct and we make sure that we also have a variable that only penalizes the classifier for missing values where the label is available. That will enable us to keep track of the error on the label training examples later on. The next part of this notebook for you to do, is to actually implement the simultaneous gradient descent that we'll use to train again. In the previous DCGAN project, we provided this part for you. So now you get to learn a little bit more about some of the nuts and bolts of building the GAN. First, you'll need to get a list of all the weights and biases for the generator. And then separately, get a list of all the weights and biases for the discriminator. Each of those sets of parameters, those for the generator and those for the discriminator, will have different updates. So you need to keep them apart from each other. Next, you'll use Adam to propose an update for each of the players. Simultaneously, each of these updates will be called and each update will minimize one player's cost with respect to that player's parameters. You should store these update opts as d_train_opt and g_train_opt. Finally, we include a shrink_lr variable, where we change the learning rate. We'll call this at the end of each epoch to make the learning rate gradually decrease over time. You do not actually need to implement anything in the GAN class itself. This just calls the functions that we've implemented earlier to set up the loss function, set up the optimizer and so on. And, it's just a place where we glue together a lot of the pieces that we built earlier. You don't need to implement anything in the view sample's function, this is just some code that we use to plot the samples and show them to you later on. You don't need to implement anything in the train function, either. We'll go ahead and run everything for you in here. This is the place where we take all the pieces that you built and we glue them together. I'll walk through a little bit just to explain what's actually going on. One thing we do is we set up a saver that will actually write the variables that you learned to a checkpoint. If you'd like to take your training model and go and do something else with it, after this notebook is done, this will allow you to do so. You could do something like go and see if you can make adversarial examples for your classifier. Or, you could use it to transcribe real photos of address numbers in images. Next we make one copy of Z that we keep the same throughout the training process so, that every time we plot the samples, we're using the same Z values and any changes that you see from one epoch to the next, are due to learning rather than sampling different values of Z. We make a tensorflow session in order to run this particular experiment. Then we initialize all of the weights and biases in the network. We iterate for a predetermined number of epochs, while you're debugging you might want to change this to a smaller number but at the end, we give you enough that you can see a nice learning curve. Overall it should take less than 25 minutes to run the whole thing. Once it's ready to run, we print out the epoch number, so that you can get a little bit of a progress indicator in the notebook, during this relatively slow part of the notebook, this cell is where most of the time is going to be spent. When we compute the training error, there is a little bit of a subtlety because different mini batches can be different sizes. Most of the mini batches are going to be size 128, but the number of examples in the training set is not divisible by 128. So, the last batch of every epoch will be a different size, because of that we actually need to compute the training accuracy, keeping track of the fact that there is a different number of examples in each mini batch. For each step of training, we grab an X value or Y value and a label mask from the data set. If you wanted to build a really serious application, this is one of the few places where this notebook is actually simplifying things. In reality, you'd want to use a tensorflow Q, to be able to read this very quickly in an asynchronous fashion. But for the purpose of this SVHN problem, it's fine to just iterate over the num pi variable like this. Next, we generate the random noise for the input to the generator and num pi. This is one of the other few places where the notebook has simplified things a little bit, compared to what you do for a real application. For a real application, you can actually just use a tensorflow random number generator that would happen on GPU rather than NCPU. We go ahead and we call the d_opt variable that you made as well as the g_opt variable, both simultaneously to optimize each player's cost with respect to that player's parameters simultaneously. We just feed in all of the data that we pulled for this iteration and run it through those optimization rules. At the end of the epoch, we shrink the learning rate so that we'll use a smaller learning rate on the next epoch. We print out the training accuracy so that you can get some idea of how well you're fitting the training set. When you look at the training accuracy values that we print out, there is two things you should keep in mind; One is that we're computing the train accuracy only on the label examples, we ignore the labels that are thrown away using the zeros and the label mask variables. We do that so that you can see how well you're fitting the examples where the labels were provided, and not get distracted by how well you're magically guessing the labels on the unlabeled training examples. One other thing to keep in mind about the training accuracy variable that we print out, is that it's updated on the fly during the learning process. Every time we load a mini batch for training, we keep track of how many examples were correctly classified at the moment that we actually trained on them. Over the course of a single epoch, the model will actually improve. It will do better at the end of the epoch than at the start of the epoch. This training accuracy variable that we print out, is an average over the performance throughout the entire time of the epoch. So if we looked at a snapshot of the model at the end of the epoch at the time we actually print this number, it's accuracy at that point in time would be slightly higher than the accuracy that we print, just because the accuracy we print is an average that looks backwards in time a little bit. That might confuse you if you weren't aware of it, because sometimes you'll see that the test accuracy is actually higher than the train accuracy, that's because the test_accuracy is computed instantaneously and the train_accuracy is averaged over the course of training on a whole epoch. After we finish training on epoch, we iterate through the whole test set and we compute the accuracy on the test set, just so that we can add that to our learning curve and see how much we were over fitting over time. We also spit out some samples that came from the generators, so that you can see how the samples adapt over time from one epoch to the next. And at the very end of the epoch, we store the accuracies that we computed in these lists that we'll use to make a plot later on. In this, so we go ahead and we set up the learning problem. We specify the size of the images, that's determined by the SVHN, dataset itself. We specify the size of the Z Vector. This is just a hyper parameter that you can play with, 100 seems to perform perfectly well, and it's also reasonably computationally cheap. And then we set the learning rate. The learning rate for Adam for GANs seems relatively stable across tasks. Here we're using .0003. For image net, I found that basically the same learning rate works really well, except that I changed the 3 to a 4. And that's all the more that I had to change for that task. So, this setup is somewhat portable from one task to another. This is the cell where we actually call most of the computation. So, most of the time that you run the notebook, you'll spend most of your time watching this cell execute. We allocate the dataset, we configure the batch size to be 128, and we ask the model to train for 25 epochs. During the debugging stage, you might try dropping that to like 2 epochs. At the end of the training process, we pull out the list of accuracies over time and we get some of the last samples. We go ahead and we plot the learning curves, showing how the accuracy improves on both the training set and the test set over time. After you go ahead and actually run your notebook, once you have everything implemented correctly, you should find that the test accuracy peaks at around 69-71 percent There's a lot of randomness in this notebook and we don't actually use random seeds anywhere. So, you don't always see the same results. It is possible to make it deterministic by seeding everything. I personally like to avoid doing that because when you move to real world problems where you have to use things like asynchronous training and cues that load your data, it's more or less impossible to make those deterministic anyway. So, I like to on even the small problems where it's possible to make them deterministic, I like to have people get used to the randomness that's inherent to working with deep learning on larger datasets. But you should pretty consistently see a test accuracy above 68 percent or so. At this point, you're probably wondering how well we're doing compared to other methods. One baseline we can look at is there was a NIPS 2014 paper on semi-supervised classification of SVHN and using variational methods. They used the same number of labels, 1,000 labels, that we use in this notebook, and they got 64 percent accuracy. If you take the same model that we have here and you rerun it with all the labels, I get above 80 percent accuracy pretty fast. And if you train it for longer, you might actually get better than that. I didn't actually run it for very long. There are other architectures that are larger than the ones in this network that obtain much higher accuracy, at least 98 percent accuracy and I'm not actually sure what the state of the art is for the fully supervised version of this task. All right, now it's time to go through the solution of the semi-supervised GAN on SBHN Notebook. We had you implement the global average pooling layer of the discriminator. This just needs to take the average across the spatial dimensions of the last layer of features. We can take an average by using the reduce mean app. And the most important part is that we just specify the axes to average over. Axis 0 is the batch, access 1 is the height, access 2 is the width, and access 3 is the channels of the feature map. So, we want to take the average over access 1 and access 2, to get rid of those spatial dimensions. The next thing that we had you implement was the logits for the different classes of the classifier. And this part, there is actually a decision that you could make about how you want to design the model. We want to have a total of 11 different class probabilities, and we represent that using a softmax. One thing that's interesting about a softmax is that it's over parameterized. The softmax normalizes itself so that it always has an output that sums to 1. That means if you know the probabilities of the first 10 classes, the 11th class is just one minus those first 10 probabilities. In terms of the logits, this means that you can set one of the logits to zero, and the other 10 logits can control the probability distribution over all 11 classes perfectly fine. So, you have a choice of whether you want to actually have a map mall that outputs 11 values, or a map mall that outputs 10 and then set the extra one to zero. We actually implemented both both versions so that you can see how they both work. So, here we just say that we're going to pay attention to this extra class switch to decide whether we should output 10 values or 11. The next thing that we had you implement is the logits for the Sigmoid value. We start off by remembering that the softmax needs to represent a distribution over the 10 real classes and the one fake class. The way that it does this will change depending on whether we've decided to pursue the 11 class softmax, or the 10 class softmax strategy. If we're pursuing the 11 class softmax strategy, then we need to actually split into two sets of logits. We have 10 different logits giving us the distribution of the real classes, and then we have one extra logit for the fake class. And all of those actually come out of the map mall and all of them are independently parameterized. If we decided to only do a 10 class map mall, then we can actually just take the output of the map mall, and that gives us our largest for the real classes. We also just hard class the logit for the fake class to zero. And then the normalization of the softmax will decide how much probability mass gets assigned to the fake class. We want to actually compute the log-sum-exp of all of the logits corresponding to the real class, and then subtract off the logit corresponding to the fake class. When we do this log-sum-exp, we're going to subtract off the max like we described in the trick for numerical stability up above. And then we need to add that max back on later so that we don't change the final result. We want this expression to be algebraically equivalent to the naive log-sum-exp. But we just want it to be rearranged into that numerically stable form. Next, we had you implement the loss for the discriminator. This is divided into two different parts. The first part is the unsupervised part that I've highlighted here. The unsupervised part, is divided into two different terms. One is the loss on the real data, and the other is a loss on the fake data. In both cases, we're dealing with a binary classification problem. So, we use the Sigmoid Cross entropy app to compute the loss. For the real data, the labels are all ones because we want to say that all the real data is real. And, for the fake data the labels are all zeros because we want to say all the fake data is fake. In both cases, we just plug in the logits that we computed up above using that log-sum-exp trick. We chose these logits up above so that if you take the Sigmoid of them, they give you the probability that the data is real rather than fake. So, they're the correct thing to use here for this binary real versus fake classification problem. The second part of the loss for the discriminator is the supervised portion of the semi-supervised learning algorithm. Here, we actually take the cross entropy in terms of the softmax between the labels for which class is present, and the logits output over all the different classes. One really important thing here is we need to remember to actually pay attention only to the labels that the label mask tells us we're allowed to say we're using. We get the cross entropy for each example in the mini-batch. And then we multiply by the label mask to make sure that we zero out the cross entropy for any of the examples where we're supposed to ignore the label. Then we want to take the mean cross entropy, but we can't just use the tf.reduce_mean app, because there is a mini-batch full of 128 examples here. But most of them in an average mini-batch, maybe even none of them, have a label mask of one. Instead, we get a different number of labeled examples that we can actually use in each mini-batch. So, we've got to explicitly compute the numerator of the mean by multiplying the label mask by each of the individual cross entropy switch example. And then compute the denominator of that mean by adding up the number of ones in our mini-batch of label masks. Finally, there's one other little trick going on here. If we have a mini-batch where nothing is labeled, and here we're using so few labels that a lot of our mini-batches are completely unlabelled, we'd end up dividing by zero, if all we had was this reduce sum here. So, we take the max between the sum and one. If the sum is zero, then we divide by one, and that doesn't actually change the value from the numerator. So we either get a numerator of zero over one, or we get a numerator of the sum of all the cross entropies divided by the number of cross entropies we added up. After we've computed both the supervised loss and both terms of the unsupervised loss, we add all three of them together to get the complete loss for the discriminator. Next we use the feature matching loss for the generator. In statistics terminology, when we take a bunch of statistics from one dataset and a bunch of statistics from another dataset and we ask those statistics to be similar, that's a learning technique called Moment Matching. So, each of the statistics that we extract is called a Moment. Here, the moments are just the average values of features from the last layer of the discriminator. First, we compute the moments on the data set itself by taking the mean across the mini-batch of all the features that we pulled out of the discriminator. Next, we compute the same moments in the same way but for the distribution of values that come from the generator, rather than from the training dataset. Finally, we compute the mean absolute difference between these two sets of moments, and we use that as the loss for the generator. That will encourage the generator to make sure that all of the feature values and the discriminator have approximately the same average value, regardless of whether the discriminator is run on the input or run on generated samples. Over time, this will force the generated samples to become very similar to data samples and have all of the statistics be the same. If any of the statistics don't match, the discriminator can learn to pay attention to different statistics in order to find the difference between the two distributions. Finally, we compute the mean absolute difference between these two sets of moments, and we use that as the loss for the generator. That will encourage the generator to make sure that all of the future values in the discriminator have approximately the same average value, regardless of whether the discriminator is run on the input or run on generated samples. Over time, this will force the generated samples to become very similar to data samples and have all of the statistics be the same. If any of the statistics don't match, the discriminator can learn to pay attention to different statistics in order to find the difference between the two distributions. We also asked you to implement the learning algorithm where we actually minimize each player's cost. To do this, you first have to understand which variables are associated with each player. We can get these two separate lists of variables by sorting the list of all trainable variables into two different lists. Because we used variable scopes, we know that all the variables for the discriminator begin with the name, "Discriminator," and all the variables for the generator begin with the name, "Generator." It's also a good idea to check and make sure that we got all of the variables. Sometimes you might accidentally write a little bit of code outside of one of these scopes and you might overlook something. So we have an assert to make sure that all of the trainable variables belong to one of these lists or the other. The next part of that we asked you to implement is the actual learning roles for each of the two players. In both cases, you simply use the AdamOptimizer that's built into tensorflow and you parse through the learning rate and Beta1 parameters that have been provided earlier in the notebook. It's important that each player minimizes the loss with respect to that player's parameters. So when we call Adam on the loss for the discriminator, we ask it to optimize only the variables associated with the discriminator. And likewise when we call Adam on the loss for the generator, we ask it to optimize only the variables associated with the generator. If we didn't include these var_list arguments, Adam would optimize all the trainable variables by default. After you've implemented everything, when you run the notebook, Cell 18 is where most of the action happens. You can see it print out the Epoch number over time, and then about 45 seconds after each Epoch starts, it should print out the classifier training accuracy, the classifier test accuracy and the amount of time that the last step took. You can use this to get an idea of how much things like the size modal players affect the runtime of your notebook. You should also see the time per Epoch, which is maybe a little bit more useful for gauging how long it will be before you see the next batch of samples. At the end of each Epoch, you should also see the samples coming up. Every time you run this notebook, you'll get slightly different results. The run that I'm showing here is maybe a little bit worse than average, just so that you don't necessarily set too high of a target for yourself. As we scroll through the history of what comes out of the training process, you can see that over time the samples should become more recognizable as digits. And also you should see that both the training accuracy and the test accuracy increase over time, and eventually plateau. After a while, we should start to see that the training accuracy is quite a lot higher than the test accuracy. This is because we're using so few label training examples. It's a lot of work for the classifier to manage to generalize accurately. On this run through the notebook, I found that the accuracy peaked at about 69% on the test set. And in a real application, you shouldn't look at the test accuracy at every time step like this. You should use a validation set to decide when to stop and then look at the test set. But in order to run this notebook faster, we only wanted to compute two different values, train and test accuracy. In general, you should see that you achieve a peak around this kind of value. It should be somewhere between like 68 to 71 or 72% accuracy.I have seen a wide range of values every time I rerun it. You should also see that you have some kind of recognizable but not necessarily high quality digits at the end of training. This set up has mostly been optimized to get good semi-supervised learning performance rather than to get good samples. Getting really good samples is actually a lot harder than getting good semi-supervised test accuracy. It's one of the reasons that I think this is one of the more useful things we can teach you about GANs at Udacity. You can easily use GANs to do semi-supervised learning, even if the learning process is not all that stable, even if the images at the end are not all that great. The images just need to be good enough to give the classifier a little bit of extra practice. And then it can begin to bring its test accuracy up and be useful for you as a classifier. If you look at the learning curves, you should see that the training set accuracy goes up really fast and eventually levels off somewhere near perfect. You'll see that the test accuracy actually levels off a lot earlier. But it's still much higher than it would be down around the 50% mark, if we were actually not using semi-supervised learning. All right, that's the end of the solution to the semi-supervised GAN notebook. You've seen that you have a really powerful tool for regularizing classifiers. And don't forget that in this notebook, we've simplified things so that the whole process runs in 25 minutes once you have it set up. If you want to get more performance, don't forget that there is also the GitHub repository linked at the end of the notebook, where we show you how to get above 94% accuracy on this task. This code is very easy to adapt and you can go and take it and apply it to any task where you don't have enough labels and you'd still like to build a train classifier that generalizes really well. So, I hope this is useful to you and I hope you take it and go and do great things.
