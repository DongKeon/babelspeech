{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 바벨스피치 Part4 - 1회차 스터디 안내\n",
    "\n",
    "- [바벨스피치 스터디 github 주소](https://github.com/KaggleBreak/babelspeech)\n",
    "- [바벨스피치 스터디 자료 보관 구글드라이브 주소](https://drive.google.com/drive/folders/0B2l0iH28o85xRG5KSHM3QlpiRjg)\n",
    "- [바벨스피치 스터디 스케줄](https://docs.google.com/spreadsheets/d/1vO5M5ygTE9P0TAMSpgR3Aw4Y_tO4l59ZzOqbcNKyphs/edit?usp=drive_web)\n",
    "- [바벨피쉬 페이스북](https://www.facebook.com/groups/babelPish/)\n",
    "- [캐글뽀개기 페이스북](https://www.facebook.com/groups/kagglebreak)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 스터디 전체 커리큘럼 \n",
    "\n",
    "- Natural Language\tNatural Language Processing (Course materials for Georgia Tech CS 4650 and 7650, \"Natural Language\")[https://github.com/jacobeisenstein/gt-nlp-class]\n",
    "- 음성인식 기초\tSpoken Language Processing: A Guide to Theory, Algorithm and System Development\n",
    "- Generative Model\tGAN 논문 모음 (https://github.com/wiseodd/generative-models)\n",
    "- 케라스 실습 (https://github.com/eriklindernoren/Keras-GAN)\n",
    "\n",
    "\n",
    "## 1. NL Intro\n",
    "\n",
    "- CS 4650 and 7650\n",
    "- Course: Natural Language Understanding\n",
    "- Instructor: Jacob Eisenstein\n",
    "- Semester: Spring 2018\n",
    "\n",
    "- 자연 언어 처리를 위한 최신 데이터 중심 기법에 대한 개요를 제공합니다. 이 과정은 얕은 bag-of-words 모델에서 단어를 상호 작용시켜 의미를 창출하는 풍부한 구조적 표현으로 이동합니다.\n",
    "- 각 수준에서, 우리는 두드러진 언어 phemonena와 가장 성공적인 컴퓨터 모델을 논의 할 것입니다. 그 과정에서 우리는 자연어 처리와 특히 관련이있는 기계학습 기술을 다루게 될 것입니다.\n",
    "\n",
    "- 전제 과목 : 알고리즘 설계 및 분석, 동적 프로그래밍, 오토마타 및 공식 언어 이론\n",
    "\n",
    "\n",
    "## 2. 목차\n",
    "\n",
    "![](./img/img1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Introduction\n",
    "\n",
    "- 자연어 처리는 컴퓨터에서 인간 언어를 액세스 할 수 있게 하는 일련의 방법\n",
    "- 자동 기계번역 \n",
    "- 텍스트 분류 \n",
    "- 검색 엔진 \n",
    "- 대화 시스템 \n",
    "\n",
    "- 이러한 다양한 응용 프로그램은 알고리즘, 언어학, 논리, 통계 등을 바탕으로 일반적인 아이디어 세트를 기반으로 함\n",
    "\n",
    "\n",
    "### 1.1 Natural language processing and its neighbors\n",
    "\n",
    "- 이 분야에서 일하는 큰 즐거움 중 하나는 언어학에서부터 통계 물리학에 이르기까지 다른 많은 지적 호기심을 활용할 수 있는 기회\n",
    "- 전산 언어학 (Computational Linguistics) \n",
    "- 언어학과 중첩되는 부분이 많지만 초점면에서 중요한 차이가 있음\n",
    "\n",
    "#### - 자연어 처리의 목표는 텍스트에서 정보 추출, 언어 간 번역, 질문에 답하기, 대화하기, 지시하기 등과 같이 인간 언어에 대한 새로운 계산 기능을 제공하는 것\n",
    "\n",
    "- 자연어 처리에 대한 현대적인 접근 방식은 기계학습에 크게 의존 \n",
    "- 기계학습은 하나의 어휘에서 이산 토큰의 시퀀스를 다른 어휘에서 일련의 이산 토큰으로 변환하는 것\n",
    "- 이미지 또는 오디오와 달리 텍스트 데이터는 근본적으로 분리되어 있음\n",
    "- 단어 집합은 분리되어 있지만 새로운 단어가 항상 만들어집니다. 매우 빈번한 단어 몇 개와 드문 단어의 긴 꼬리가 있을 것\n",
    "\n",
    "#### - 결과적으로 자연어 처리 알고리즘은 훈련 데이터에서 발생하지 않는 관측에 특히 강력해야 함 (unseen 데이터)\n",
    "\n",
    "- 언어는 재귀적, 단어와 같은 단위를 결합하여 문구를 만들 수 있음. \n",
    "- 예를 들어, 명사구는 백색 고래처럼 작은 명사구와 전치사구를 결합하여 만들 수 있습니다. 전치사 구는 전치사를 다른 명사구(고래)와 결합하여 만들어집니다. \n",
    "\n",
    "> (. . . huge globular pieces of the whale of the bigness of a human head)\n",
    "\n",
    "```\n",
    "- 문구의 의미는 근본적인 계층적 구조에 따라 분석되어야 함. 고래의 거대한 구형 조각은 인간 머리의 bigness의 전치사구와 결합되는 단일 명사구로 작용함\n",
    "- 대신 거대한 구형 조각이 인간 머리의 bigness의 고래의 전치사구와 결합되어 실망스럽게 작은 고래를 암시한다면 해석은 달라질 것이다.\n",
    "```\n",
    "\n",
    "- 텍스트가 시퀀스로 표시 되더라도 기계학습 방법은 암시적 재귀 구조를 설명해야 함. \n",
    "- 인공지능의 목표는 인간과 같은 범위의 능력을 가진 소프트웨어와 로봇을 만드는 것\n",
    "- Turing의 1949 년 논문 Computing Machinery and Intelligence로 돌아가 Turing 테스트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Winograd 스키마\n",
    "\n",
    "- Winograd 스키마는 하나의 단어가 대명사의 가능한 지시자를 바꿔 지식과 추론을 필요로하는 것처럼 보이는 예이다 \n",
    "\n",
    "> (1.2) The trophy doesn’t fit into the brown suitcase because it is too [small/large].\n",
    "\n",
    "- 마지막 단어가 small이면 대명사는 가방을 나타냅니다. \n",
    "- 마지막 단어가 large이면 트로피를 나타냅니다. \n",
    "- 이 예제를 해결하려면 공간 추론이 필요함. 다른 스키마는 행동과 그 영향, 감정과 의도, 사회적 관습에 대한 추론을 필요로 함\n",
    "\n",
    "\n",
    "#### 여러 도메인에서 자연어 처리\n",
    "\n",
    "- 자연어 처리, 기계 학습 및 컴퓨터 비전과 같은 하위 분야의 연구가 점차 증가하고 있음, 누구나 전 분야에서 전문 지식을 유지하는 것이 어려움\n",
    "- Chomsky와 Montague와 같은 언어 학자들은 형식 언어 이론이 자연어의 구문과 의미를 설명하는 데 도움이 되는지 보여주었음\n",
    "- 레이블이 없는 텍스트의 대형 데이터 세트는 MapReduce 와 같은 병렬화 기술에 대한 자연스러운 응용 프로그램\n",
    "- 소셜 미디어와 같은 대용량 데이터 소스는 대략적인 스트리밍 및 스케치 기술에 자연스럽게 적용됨\n",
    "- 음성 처리 자연어는 대개 음성으로 전달되며 음성 인식은 오디오 신호를 텍스트로 변환하는 작업입니다. \n",
    "- 음성 인식은 텍스트 분석과 통합되는 경우가 많음. 특히 텍스트 시퀀스의 확률을 정량화하는 통계 언어 모델을 사용하는 경우가 많음 (6 장 참조) \n",
    "- 음성 인식 외에도 광범위한 음성 처리 분야에는 19 장에서 간략히 논의되는 음성 기반 대화 시스템에 대한 연구가 포함됨 \n",
    "- 기타 자연 언어 처리는 전산 사회 과학 및 디지털 인문학과 같은 신흥 학제 분야에서 중요한 역할을 함, 텍스트 분류 (4 장), 클러스터링 (5 장) 및 정보 추출 (17 장)은 특히 유용한 도구\n",
    "- 또 하나는 확률론적 주제 모델. 정보 검색 (Manning et al., 2008), 잠재적 의미 해석 (§ 14.3)과 같은 기법\n",
    "- 텍스트 마이닝은 데이터 마이닝 기술, 특히 분류 및 클러스터링을 텍스트에 적용하는 것을 가리키는 데 종종 사용됨 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Three themes in natural language processing\n",
    "\n",
    "- 자연어 처리의 세 가지 주제 \n",
    "\n",
    "#### Learning and knowledge\n",
    "#### Search and learning\n",
    "#### Relational, compositional, and distributional perspectives\n",
    "\n",
    "\n",
    "\n",
    "## 1.2.1 Learning and knowledge\n",
    "\n",
    "- 원시 텍스트를 원하는 출력 구조 (예 : 요약, 데이터베이스)로 변환하는 엔드 투 엔드 시스템을 훈련하기 위해 기계학습을 사용\n",
    "- 다른 극단에서는 자연어 처리의 핵심 작업이 텍스트를 일반 목적의 언어 구조 스택으로 변환하는 데 사용\n",
    "- 형태소 분석이라는 하위단어 단위에서 단어 수준 품사체, 트리구조 표현 문법의 의미를 넘어 논리 기반의 의미 표현 \n",
    "\n",
    "- 엔드 투 엔드 학습 접근법... 음운론의 기본을 토대로 전문가가 작성한 표현이 사라짐\n",
    "    - 구문 트리와 같은 언어 표현\n",
    "\n",
    "- 언어학자는 언어의 이해와 생산을 용이하게 하기 위해 특별히 고안된 추상화를 인코딩하는 모든 인간에 \"언어 교수\"의 존재 주장 \n",
    "- 단어에 첨부 할 수 있는 접미사 시스템의 복잡성에서 영어가 아닌 다른 많은 언어에서 특히 중요함\n",
    "- 대안적... 파서 (parser) 또는 품사 태거 (tagger)와 같은 범용 언어 처리 시스템의 출력으로부터 획득 될 수 있음\n",
    "- 학습과 지식의 또 다른 종합은 모델 구조에 있습니다. \n",
    "\n",
    "- 예를 들어, 문장의 구성은 종종 소규모 구성 요소의 의미로부터 점차적으로 구성되는 큰 단위의 의미와 함께 구성 적으로 설명됩니다. \n",
    "\n",
    "- 기계 학습 전문가는 공학적 방법론이 비 과학적 연금술이라는 말을 듣기를 좋아하지 않음, 언어학자는 일반적인 언어 원리와 구조에 대한 검색이 빅데이터. 그러나 두 가지 유형의 연구를 위한 명확한 여지가 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.2 Search and learning\n",
    "\n",
    "![](./img/img2.png)\n",
    "\n",
    "- 예를 들어, 입력 x는 소셜 미디어 게시물, 출력 y는 저자가 표현한 정서적 감정의 레이블 (4 장)\n",
    "- 또는 x는 불어로 된 문장 일 수 있음, 출력 y는 타밀 (Tamil)의 문장 일 수 있음 (18 장)\n",
    "- 또는 x는 영문 문장 일 수 있고, y는 문장의 구문 구조 표현 일 수있다 (10 장)\n",
    "- 또는 x는 뉴스 기사 일 수 있고 y는 기사에서 설명하는 이벤트의 구조화 된 기록 일 수 있음 (17 장)\n",
    "\n",
    "- 이 공식을 채택함으로써 언어처리 알고리즘이 검색이라는 두 가지 모듈을 갖게 된다는 암시적 결정\n",
    "- 검색 모듈은 함수 Ψ의 argmax를 계산함, 즉, 입력 x와 관련하여 최상의 점수를 얻는 출력 y를 찾음 \n",
    "- 검색 공간 Y (x)가 열거하기에 충분할 때 또는 스코어링 함수 Ψ가 부분들로의 편리한 분해를 가질 때 용이 \n",
    "- 많은 경우, 우리는 이러한 속성을 갖지 않는 채점 함수를 사용하여 보다 정교한 검색 알고리즘의 사용을 유도하고자 함\n",
    "- 출력은 일반적으로 언어 처리 문제에서 불연속이기 때문에 검색은 종종 조합 최적화의 기계에 의존합니다. \n",
    "\n",
    "- 학습 모듈은 매개 변수 θ를 찾는 역할. 이는 일반적으로 레이블이 지정된 예제의 대형 데이터 세트를 처리하여 수행됨 (항상 그런 것은 아닙니다)\n",
    "- 탐색과 마찬가지로 학습은 2 장에서 볼 수 있듯이 최적화 프레임 워크를 통해 접근"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.3 Relational, compositional, and distributional perspectives\n",
    "\n",
    "- 언어의 모든 요소 - 단어, 구, 문장 \n",
    "- 언론인은 직업의 하위 카테고리, 앵커는 언론인의 하위 카테고리. 또한 기자는 저널리즘을 수행. 저널리즘은 흔히는 아니지만 종종 글쓰기의 하위 범주.\n",
    "- 단어와 다른 기본 의미 단위 사이의 관계를 열거하는 Word-Net (Fellbaum, 2010)과 같은 의미론적 온톨로지의 기초\n",
    "- 그러나 관계형 관점의 추론력에도 불구하고 계산적으로 형식화하는 것은 쉽지 않음\n",
    "\n",
    "- 온톨로지 구축의 어려움으로 인해 일부 언어 학자들은 단어 의미를 분리하는 작업은 독립적 방법이 없다고 주장하기도 함\n",
    "- 몇 가지 문제가 더 쉽습니다. 기자 그룹의 각 구성원은 언론인. -s 접미사는 영어로 된 명사 대부분에서 단수와 복수를 구분\n",
    "- 마찬가지로 언론인은 대개 구어체로 일기를 제작하거나 일하는 사람으로 생각할 수 있음, 단어의 의미는 구성 요소 - 구성의 원칙으로 구성됩니다.\n",
    "\n",
    "> (1.4) The blubber served them as fuel.\n",
    "\n",
    "> (1.5) . . . extracting it from the blubber of the large fish . . .\n",
    "\n",
    "> (1.6) Amongst oily substances, blubber has been employed as a manure.\n",
    "\n",
    "- 이러한 문맥은 단어 blubber의 분포 속성을 형성하고, 유사한 구조로 나타날 수 있는 단어, 즉 뚱뚱한, 털이 많은, barnacles와 연결됨\n",
    "- 이러한 분배 관점은 레이블이 없는 데이터만으로 의미에 대해 배울 수 있게 해줌\n",
    "- 관계형 및 구성적 의미와 달리 수동 주석이나 전문가 지식이 필요하지 않음. 따라서 분산 의미론은 거대한 범위의 언어 현상을 포괄 할 수 있음\n",
    "\n",
    "- Relational, compositional, and distributional perspectives 관점은 언어적 의미에 대한 이해에 기여하며,이 세 가지 모두 자연어 처리에 중요합니다. \n",
    "- 그러나 그들은 겉보기에는 양립 할 수없는 표현과 알고리즘 접근법을 요구하는 불안한 협력자\n",
    "- 이 텍스트는 이러한 각각의 표현을 사용하여 잘 알려지고 가장 성공적인 방법을 제시하지만 미래의 연구가 이들을 결합하는 새로운 방법을 제시 할 것으로 기대됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Learning to do natural language processing\n",
    "\n",
    "- 자연어 처리에있어 주목할만한 점은 검색과 같은 여러 가지 방법으로 많은 문제를 해결할 수 있다는 것\n",
    "\n",
    "- Naive Bayes, 로지스틱 회귀, 퍼셉트론, 기대 최대화, 행렬 인수 분해, 역전파, RNN. 이 텍스트는 이러한 방법이 어떻게 작동하는지, 문서 분류, 단어 감별, 시퀀스 라벨링 (품사 태깅 및 명명 된 엔티티 인식), 구문 분석 및 해석과 같은 자연어의 컴퓨터 처리에서 발생하는 문제에 어떻게 적용될 수 있는지 설명. 담화 분석, 언어 모델링, 기계 번역 등을 포함\n",
    "\n",
    "\n",
    "## 1.3.1 Background\n",
    "\n",
    "- 자연어 처리는 여러 가지 다른 지적 전통에 의존하기 때문에 접근하는 사람은 누구나 어떤 방식 으로든 준비 부족하다고 느낌\n",
    "- 수학 및 기계 학습. 미적분 및 선형 대수학\n",
    "- 확률과 통계. 기본 확률에 대한 재검토는 부록 A에 있으며 수치 최적화에 대한 최소 검토는 부록 B에 나와 있음. 선형 대수학의 경우 Stran의 온라인 과정과 교과서 (2016)은 리뷰 자료 훌륭한 출\n",
    "- Deisenroth et al. (2018)은 현재 기계 학습을 위한 수학에 대한 교과서를 준비하고 있으며 온라인에서 여러 장을 찾을 수 있습니다 \n",
    "- 언어학. 이 책은 영어 문법 연구에서 아마도 겪었던 명사와 동사를 비롯한 초등 개념을 제외하고는 언어학에 대한 공식 교육을 전제로 하지 않음. \n",
    "- 형태론과 구문 (9 장), 의미 (12 장과 13 장), 담론 (16 장). 언어 문제는 응용 프로그램 중심의 4, 8 및 18 장에서도 발생\n",
    "- Bender (2013)는 자연어 처리 학생을위한 언어학에 대한 간단한 안내서를 제공\n",
    "- 컴퓨터 과학. 이 책은 알고리즘과 복잡성 이론의 분석에 대한 입문 과정을 수강했다고 가정되는 컴퓨터 과학자를 대상으로 함.\n",
    "- 특히 알고리즘의 시간과 메모리 비용에 대한 점근 적 분석에 익숙해야 하며 동적 프로그래밍을 알아야 함. 알고리즘에 관한 고전적인 텍스트는 Cormen et al. (2009); \n",
    "- 계산 이론에 대한 소개는 Arora and Barak (2009)와 Sipser (2012)를 참조하십시오."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.2 How To use this book\n",
    "\n",
    "- Learning. 이 섹션에서는 나머지 교과서에서 사용되는 일련의 기계 학습 도구를 작성합니다. \n",
    "    - 기계학습에 중점을 두고 있기 때문에 텍스트 표현과 언어 현상은 대부분 간단합니다. \"단어 봉합 (bag-of-words)\"텍스트 분류가 모델 예제로 취급됩니다.\n",
    "\n",
    "- 시퀀스와 트리, 4 장에서는 단어 기반 텍스트 분석의보다 언어 학적으로 흥미로운 적용에 대해 설명\n",
    "    - 이 섹션에서는 구조화 된 현상으로서의 언어 처리를 소개합니다. 시퀀스 표현과 트리 표현 및 이들이 용이하게 사용하는 알고리즘과 이러한 표현이 부과하는 한계를 설명합니다. \n",
    "    - 9 장에서는 유한 상태 오토마타 (finite state automata)를 소개하고 문맥이없는 영어 구문에 대한 간략한 개요를 설명합니다. \n",
    " \n",
    "- 의미와 밀접한 관련이있는 두 가지 주제, 즉 모호한 참조의 해결과 다중 문장 담화 구조 분석을 포함. \n",
    "    - 응용 프로그램. 마지막 섹션에서는 자연어 처리의 가장 두드러진 응용 프로그램 인 정보 추출, 기계 번역 및 텍스트 생성에 대해 장 길이의 처리 방법을 제공합니다. \n",
    "    - 이러한 각 응용 프로그램은 독자적으로 교과서 길이의 치료를 받을 수 있습니다 (Koehn, 2009, Grishman, 2012, Reiter and Dale, 2000). 여기에서 챕터는 신경주의와 같은 방법을 소개하는 동안 형식과 방법을 사용하여 가장 잘 알려진 시스템 중 일부를 설명합니다.\n",
    "\n",
    "\n",
    "- 각 장에는 별표가 표시된 고급 재료가 포함되어 있음. 이 자료는 나중에 오해를 일으키지 않고 안전하게 생략 할 수 있음. \n",
    "    - 그러나 이 고급 섹션이 없어도 텍스트는 한 학기 동안 너무 길기 때문에 강사는 각 장을 골라야 함. \n",
    "    - 2 장과 3 장은 책 전체에 사용될 빌딩 블록을 제공하고 4 장은 언어 기술 실습의 몇 가지 중요한 측면을 설명합니다. \n",
    "    - 언어 모델 (6 장), 순서 표시 (7 장) 및 구문 분석 (10 장과 11 장)은 자연어 처리의 표준 주제이며 분산 단어 삽입 (14 장)은 매우 유비 쿼터스하기 때문에 학생들은 배제하면 불평 할 것입니다 . \n",
    "    - 응용 프로그램 중 기계 번역 (18 장)이 최선의 선택입니다. 정보 추출보다 응집력이 있으며 텍스트 생성보다 성숙합니다. 저의 경험에 따르면, 거의 모든 학생들은 부록 A에있는 확률에 대한 복습의 혜택을 볼 수 있습니다.\n",
    "\n",
    "\n",
    "- 기계 학습에 초점을 둔 코스는 감독되지 않은 학습에 대한 장을 추가해야합니다 (5 장). \n",
    "- 술어, 인수 의미 (13 장), 참조 분석 (15 장) 및 텍스트 생성 (19 장)에 대한 장은 특히 깊은 신경 네트워크 및 검색 학습을 포함한 최근의 기계 학습 혁신의 영향을 받습니다.\n",
    "- 좀 더 언어 지향적 인 코스는 시퀀스 라벨링 (8 장), 공식 언어 이론 (9 장), 의미론 (12 장 및 13 장), 담화 (16 장)의 적용에 대한 장을 추가해야합니다. \n",
    "- 학부생을 대상으로 한 코스와 같이 좀더 집중적 인 코스의 경우 - 시퀀스 라벨링 (8 장), 술어 - 인수 의미론 (13 장), 정보 추출 (17 장) 및 텍스트 세대 (19 장)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
